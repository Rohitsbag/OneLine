REST API
0:00
REST API. What is a REST API? Well,
0:05
think of REST API like a waiter at a
0:07
restaurant. You tell the waiter what you
0:09
want, they go to the kitchen, get it,
0:10
and bring it back to you. That's exactly
0:12
what a REST API does between your app
0:14
and a server. REST stands for
0:17
representational state transfer. Fancy
0:20
words that basically mean a simple way
0:22
for applications to talk to each other
0:24
over the internet. REST uses the same
0:26
HTTP methods you already know from web
0:29
browsing. Get to retrieve data like
0:32
asking show me all users. Post to create
0:35
something new like add this new user.
0:38
Put to update existing data and delete
0:41
to remove it. Let's say you're building
0:43
a social media app. When you want to
0:46
show user profiles, your app sends a get
0:48
request to something like
0:50
api.mmyapp.com/users/1
0:54
and boom, you get back all of J's
0:57
profile info in a nice, clean JSON
1:00
format. REST is stateless, means each
1:03
request is completely independent. The
1:05
server doesn't remember your previous
1:07
requests, which means it can handle
1:09
millions of users without getting
1:10
confused about who asked for what. Rest
1:14
is also platform independent. Your
1:16
iPhone app, Android app, web app, and
1:19
even your smart fridge can all talk to
1:21
the same REST API. But REST isn't formal
1:24
enough for your bank transfers and
1:26
enterprise systems. That's where SOAP
1:29
API comes in. SOAP API. SOAP or simple
SOAP API
1:33
object access protocol is one of the
1:36
oldest and most formal ways that systems
1:38
communicate with each other. If REST is
1:41
like a casual phone call, SOAP is more
1:43
like a formal business contract. Every
1:45
message must follow strict rules and
1:48
comes wrapped in XML with a very
1:50
specific structure, an envelope to wrap
1:53
everything, a header for metadata, and a
1:55
body that holds the actual request or
1:57
response. One of SOAP's strengths is
2:00
that it's protocol independent. While
2:02
it's most commonly used over HTTP or
2:05
HTTPS,
2:06
it can also run on SMTP, TCP, or other
2:10
protocols. And because it has built-in
2:12
standards for error handling, security,
2:15
and transaction support, SOAP is often
2:18
trusted in industries where reliability
2:20
and precision matter most. That's why
2:23
banks, healthcare providers, and
2:25
government systems still rely heavily on
2:27
SOAP today. For example, when you
2:30
transfer money between banks, there's a
2:32
good chance a SOAP API is working behind
2:35
the scenes, ensuring the transaction is
2:37
secure and correctly processed. SOAP
2:40
might not be as lightweight or flexible
2:42
as REST, but when you need guaranteed
2:44
delivery, strict contracts, and
2:46
enterprisegrade reliability, SOAP is the
2:49
go-to choice. GRPC API. Before we talk
gRPC API
2:53
about gRPC, let's step back to RPC or
2:56
remote procedure call. RPC is the idea
2:59
that instead of sending raw data over
3:01
the network, your app can directly call
3:03
a function on another machine as if it
3:05
were local. For example, you write get
3:08
user 123 in your code and behind the
3:11
scenes that request travels across the
3:12
network, runs on the server and returns
3:15
the result. But early RPC systems like
3:18
XML, RPC or JSON RPC had problems. They
3:22
were slower, textheavy and didn't scale
3:24
well for today's massive realtime apps.
3:28
That's where gRPC comes in. GRPC is
3:31
Google's higherformance modern take on
3:33
RPC. Think of it as the Formula 1 race
3:36
car of APIs built for speed,
3:38
performance, and precision. While REST
3:41
sends textbased JSON over HTTP, gRPC
3:45
uses protocol buffers or protobu, which
3:48
compress data into a compact binary
3:50
format that's lightning fast to process.
3:53
It's like the difference between mailing
3:55
a handwritten letter versus zipping all
3:57
your files and sending them instantly.
4:00
GRPC also takes advantage of HTTP2,
4:03
allowing multiple requests to run over a
4:06
single connection at the same time. And
4:09
here's the best part. GRPC supports four
4:11
powerful communication patterns. Simple
4:14
request response just like REST. Server
4:17
streaming for live updates, client
4:19
streaming for sending continuous data.
4:22
and birectional streaming where both
4:24
sides can chat at once in real time. The
4:28
performance gains are massive, often 7
4:30
to 10 times faster than rest in many
4:32
scenarios. That's why gRPC is the secret
4:35
weapon behind systems like Netflix,
4:38
Uber, and highfrequency trading
4:39
platforms. GraphQL API. GraphQL stands
GraphQL API
4:44
for graph query language, and it's about
4:46
to change how you think about APIs
4:48
forever. Created by Facebook, GraphQL is
4:51
the game-changing query language that's
4:53
revolutionizing how we fetch data.
4:55
Here's the problem that GraphQL solves.
4:57
In the case of REST APIs, you often get
5:00
too much or too little data. Need a
5:03
user's name and email? Rest might send
5:06
you their entire profile, address,
5:08
preferences, and shopping history.
5:10
That's called overfetching, and it
5:12
wastes bandwidth. Or worse, you might
5:14
need to make multiple API calls to get
5:16
everything you need. That's
5:18
underfetching.
5:19
GraphQL flips this completely. You write
5:22
one query asking for exactly what you
5:24
want, like just give me the username and
5:26
email, skip everything else. One end
5:29
point, one request, perfect data every
5:31
time. But the killer feature of GraphQL
5:34
is real-time subscriptions. Your app can
5:36
listen for live update automatically.
5:38
Plus, GraphQL is self-documenting with a
5:41
built-in playground where you can test
5:42
queries instantly. GitHub's entire API
5:46
is built on GraphQL. Shopify and
5:49
Pinterest all use GraphQL in production.
5:52
If you're building modern applications
5:54
where performance matters, users are on
5:56
mobile devices, and you want to give
5:58
front-end developers the flexibility to
6:01
request exactly what they need. GraphQL
6:04
might just be your new best friend. But
6:06
sometimes you don't just want to fetch
6:08
data, you want to be notified the
6:09
instant something changes. That's where
6:11
web hooks come in. Web Hook API. Imagine
WebHooks API
6:15
this. With most APIs, your app is like
6:17
someone constantly checking their
6:19
mailbox. You walk outside, open it, see
6:21
nothing, and walk back over and over
6:24
again. That's how traditional APIs work.
6:27
Your app has to ask the server every
6:29
time it wants to know if something new
6:30
happened. But web hooks flip that model
6:33
completely. Instead of you asking, the
6:35
API calls you. It's like the mailman
6:37
ringing your doorbell the moment a
6:39
letter arrives. Instant, direct, and
6:41
efficient. Let's see how it works. You
6:43
set up a callback URL in your
6:45
application. Whenever an event happens,
6:48
like a new payment, a code push, or a
6:50
form submission, the service sends a
6:52
post request with the event details
6:54
straight to your callback URL. No
6:56
polling, no wasted requests, just
6:59
real-time updates. That's why web hooks
7:01
are often called reverse APIs. Instead
7:04
of your app chasing the data, the data
7:06
comes chasing your app. You'll find web
7:09
hooks powering nearly every modern
7:10
application like GitHub fires web hooks
7:13
when new code is pushed. Shopify
7:16
triggers them when an order is placed.
7:18
Slack and Discord bots rely on web hooks
7:21
for commands and real-time reactions.
7:23
From automating workflows to keeping
7:25
systems instantly in sync, web hooks are
7:28
the invisible backbone of real time web
7:30
development. Websockets API. Websockets
WebSockets API
7:34
are like opening a permanent phone line
7:36
between your app and the server. Once
7:38
the connection is established, both
7:40
sides can talk to each other anytime
7:42
instantly. No more waiting for the
7:44
client to ask a question like with
7:46
traditional HTTP.
7:48
Here's how it works. It starts with a
7:50
handshake. Your browser sends a special
7:52
HTTP request saying, "Let's upgrade this
7:55
to a websocket connection." The server
7:57
agrees. They shake hands. And from that
8:00
point on, the channel stays open. This
8:02
gives you a persistent two-way
8:04
communication line, which is perfect for
8:06
real-time applications. Unlike regular
8:09
HTTP where the client always initiates,
8:12
websockets allow the server to push data
8:14
to you the moment something happens.
8:17
Think about getting a stock price
8:18
update, a chat message, or a game event
8:20
the instant it occurs. That's the power
8:22
of websockets. And it's flexible, too.
8:26
You can send plain text, JSON for
8:28
structured data, or even binary files
8:30
like images and videos. Web RTC API. Web
WebRTC API
8:35
RTC or Web Realtime Communication isn't
8:38
just a single API. It's a full framework
8:41
that enables direct peer-to-peer
8:43
communication between browsers or mobile
8:45
apps. And here's the magic. The data
8:47
doesn't need to flow through a central
8:49
server. That's why Web RTC powers things
8:52
like video calls, screen sharing, online
8:55
gaming, and instant file transfers. All
8:57
happening right inside your browser with
9:00
no extra software.
9:02
Think about your last Zoom or Google
9:04
Meet Call. When you're talking, your
9:06
video and audio are sent straight from
9:08
your device to the other person's
9:10
device. No server in the middle storing
9:13
or processing your private conversation.
9:16
It's direct and real time. Behind the
9:20
scenes, WebRTC takes care of the messy
9:22
networking details. It figures out NAT
9:25
traversal so devices can talk across
9:27
different networks. It automatically
9:29
negotiates the best audio and video
9:31
formats. And it uses adaptive bit rate
9:34
streaming, which means it constantly
9:36
adjusts quality depending on your
9:37
internet speed. The result, no server
9:40
bottlenecks, faster communication, and
9:43
smoother realtime experiences.
9:45
That's why WebRTC is the backbone of
9:48
modern video conferencing, realtime
9:50
collaboration tools, and peer-to-peer
9:52
apps.

Intro
0:01
Hey everyone! Are you tired of slow  APIs making your application sluggish?
0:12
You're in the right place! In today's video,  
0:15
we're going to explore 7 optimization techniques  that can help your APIs perform at their best.
0:22
We've all been there - you've built  an amazing API, but it's just not as  
0:27
fast as you'd like it to be. But  don't worry, we're here to help!
0:32
Before we start, it's important to note that  optimization should not be the first step in your  
Optimization
0:37
process. Optimization is powerful, but it can lead  to unnecessary complexity if done prematurely.
0:45
The first step should always be  to identify the actual bottlenecks  
0:49
through load testing and profiling  requests. Only begin optimization  
0:55
once you've confirmed that an API  endpoint has performance issues.
1:00
With that said, let's get into the tips!
Caching
1:04
First up - caching.
1:06
This technique is one of the most  effective ways to speed up your  
1:10
APIs. By caching, we store the result  of an expensive computation so that  
1:16
we can use it again later without  needing to redo the computation.
1:20
If you have an endpoint that is frequently  accessed with the same request parameters,  
1:25
you can avoid repeated database hits by  caching the response in Redis or Memcached.
1:32
Most caching libraries make this easy to  add with just a few lines of code. Even  
1:37
a brief period of caching can make  a significant difference in speed.
1:43
Next - connection pooling. This optimization  technique involves maintaining a pool of open  
Connection polling
1:50
connections, rather than opening a new  database connection for each API call.
1:56
Creating a new connection each  time involves a lot of handshake  
2:00
protocols and setup which can slow down your API.
2:04
This reuse of connections can  greatly improve throughput.
2:08
If you're using a serverless architecture,  
Connection management
2:11
connection management can  be a bit more challenging.
2:14
This is because each serverless function instance  typically opens its own database connection,  
2:20
and because serverless can scale rapidly,  
2:23
this could potentially lead to a large number of  open connections that can overwhelm the database.
2:30
Solutions like AWS RDS Proxy and  Azure SQL Database serverless are  
2:37
designed to handle this situation and  manage connection pooling for you.
Avoid M queries
2:42
Closely related to database performance,  
2:44
our third tip is to avoid N+1 query problems.  The N+1 problem is a common inefficiency that  
2:48
can occur when accessing data of an entity  and its related entities. For example,  
2:49
let's say you're building an API endpoint to  fetch blog posts and their comments. An N+1  
2:55
problem would occur if you first made a query  to fetch the posts, and then for each post,  
3:01
you made another query to fetch its comments.  If you have N posts, this would result in 1  
3:07
query for the posts plus N queries for the  comments, hence the term "N+1 problem".
3:14
To avoid this, it's more efficient to fetch  the data in a single query, or in some cases,  
3:19
two queries: one to fetch the posts, and one  to fetch all the comments for those posts.
3:26
This avoids making a separate query for each  post's comments and can significantly reduce  
3:30
the number of round trips to the  database, improving performance.
Pagination
3:35
Moving on to our fourth tip,  consider using pagination.  
3:39
If your API response returns a large amount  of data, it can slow things down. Instead,  
3:46
break the response into smaller,  more manageable pages using limit  
3:50
and offset parameters. This can speed up data  transfer and reduce load on the client side.
Lightweight Serializers
3:58
Our fifth technique is about using  lightweight JSON serializers.
4:02
When returning JSON responses from your  API, the speed of your serialization  
4:07
process can make a noticeable difference  in response times. Consider using a fast  
4:13
serialization library to minimize the time  spent converting your data into JSON format.
Compression
4:20
For our sixth technique, we have compression.
4:23
By enabling compression on large API response  payloads, you can reduce the amount of data  
4:29
transferred over the network. The  client then decompresses the data.
4:34
Nowadays, there are even more efficient algorithms  
4:36
like Brotli that provide  better compression ratios.
4:40
Also, many Content Delivery Networks (CDNs)  like Cloudflare can handle compression for you,  
4:46
offloading this task from your server.
Asynchronous Logging
4:50
Finally, we have asynchronous logging.  
4:53
In many applications, the time it  takes to write logs is negligible.
4:58
However, in high-throughput systems  where every millisecond counts,  
5:03
the time taken to write logs can add up.
5:05
In such cases, asynchronous logging can  help. This involves the main application  
5:12
thread quickly placing the log entry into  an in-memory buffer, while a separate  
5:18
logging thread writes the log entries to the  file or sends them to the logging service.
Outro
5:23
Just keep in mind that with asynchronous  logging, there's a small chance you  
5:28
might lose some logs if your application  crashes before the logs have been written.
5:34
There you have it! Seven tips to  help make your APIs faster and  
5:38
more efficient. If you have any other  techniques that you've found useful,  
5:42
let us know in the comments. Thanks for  watching, and see you in the next video!
5:45
If you like our videos, you may like our system  design newsletter as well. It covers topics and  
5:52
trends in large-scale system design. Trusted by  450,000 readers. Subscribe at blog.bytebytego.com




0:00
Building secure and reliable APIs is  one of the most challenging aspects of  
0:04
web development. What's surprising is  that, despite working with APIs daily,  
0:08
both frontend and backend developers  often misunderstand, overlook,  
0:12
or blindly rely on libraries that abstract away  the client - server communication. [boring]
0:16
This lack of understanding leads to inconsistent  response structures, security vulnerabilities,  
0:21
and performance bottlenecks, so it is a no-brainer  that everybody involved in building modern  
0:26
applications should have the knowledge required  to design, implement, and consume APIs correctly.
0:31
The good news is that gaining this  knowledge is easier than you might expect.
0:35
While most of us think of REST when it  comes to APIs, it is worth mentioning  
0:39
that there are actually multiple paradigms,  each with its own advantages and trade-offs.
0:43
Unlike REST, which returns fixed data structures,  
0:46
GraphQL lets clients specify exactly what  data they need in a single request. This  
0:51
prevents transferring unnecessary data  over the wire, making it highly efficient.
0:55
gRPC on the other hand is a high-performance  API framework that uses Protocol Buffers  
1:00
instead of JSON for data serialization.  It supports bi-directional streaming,  
1:05
making it ideal for real-time  communication between microservices.
1:09
At the end of the day, all these solutions  address the same problem of managing inputs,  
1:13
which are the changes triggered  by user actions, and outputs,  
1:16
or the information returned when  querying resource endpoints.
1:19
In this video, we’ll focus on the most popular  solution. REST follows a stateless client-server  
1:24
architecture, using standard HTTP methods to  perform operations on resources. It’s simple,  
1:30
scalable, and widely supported, making  it the default choice in most use cases.
1:34
Imagine you're designing an API for an e-commerce  platform. In this case, an input could be a  
1:39
customer adding an item to their cart, updating  their shipping address, or placing an order.
1:44
On the other hand, outputs are what the API  returns in response to queries, like fetching  
1:48
product details, retrieving a user's order  history, or checking the current stock of an item.
1:53
If we remove all the layers of  complexity, at its core an API  
1:57
allows clients like web or mobile apps to  consume and modify data from a database.
2:02
[1. CRUD to REST] The data layer can  
2:03
mainly perform 4 operations. Following  this idea, well designed APIs map these  
2:08
database operations to clear and predictable  endpoints that follow RESTful principles.
2:12
Remember that naming consistency is  key when defining these endpoints.  
2:16
Following well-established standards,  such as the Richardson Maturity Model,  
2:20
makes your API more intuitive and easier to  work with, especially for new developers.
2:25
Let’s break this down with a practical  example to see how CRUD maps to REST in  
2:29
action. For our e-commerce platform, imagine  we’re working with a resource called products.
2:34
To add a new product to the database, you’d use  a POST request to /products. The client sends  
2:38
a JSON object with the product’s name, price,  and description as the payload, and the server  
2:43
responds with a status code like 201 Created  along with the newly created product’s ID.
2:49
To fetch product details, you’d use a GET request. 
2:52
A GET sent to the /products root path  could return a list of all products,  
2:55
while GET /products/5 retrieves the specifics  of the product with the specified id.
3:00
To modify an existing product,  like changing its price,  
3:03
you’d send a PUT or PATCH request  to the same “products” path.
3:07
The difference between actions is key here. PUT  replaces the entire resource, so you have to  
3:11
send the full updated object as the payload,  while PATCH lets you send just the changes,  
3:15
like the price for instance. The server  confirms with a 200 OK or 204 No Content.
3:21
Finally, to remove a product a DELETE request  to /products/123 should do the trick. Since no  
3:26
response body is needed, a 204 No content  response would be enough here as well.
3:31
Designing and testing APIs can be a drag,  
3:33
but that’s where today’s sponsor,  EchoAPI, comes into play!
3:36
With EchoAPI’s free toolkit set, you can spend  less time on tedious tasks like designing,  
3:41
testing, and debugging, and  more time building your product.
3:44
Plus, their new AI features are a game-changer. Say goodbye to manual updates! EchoAPI's AI  
3:49
features auto-generate parameter names, values,  and descriptions, saving you time and effort.
3:54
Let EchoAPI handle the heavy lifting,  so you can focus on what really matters!
3:58
[2. Reliability] The REST to CRUD mappings keep your API  
4:00
predictable. Developers consuming it can guess the  endpoints without digging through documentation.
4:06
But here’s where things get tricky:  consistency isn’t just about naming,  
4:10
it’s also about structuring your responses.
4:12
A common pitfall is returning inconsistent  data shapes. If two backend developers are  
4:17
working on the API and they don’t  share a common set of standards,  
4:20
product details could end up having  different structures for different endpoints.
4:24
This forces frontend devs to write  extra logic to handle the mess. 
4:28
Instead you should always keep  your DTOs consistent. And,  
4:31
of course, these rules apply to errors as well.
4:34
[3. Fine tuning your API] Consistency and reliability are essential,  
4:36
but a few easy extra steps can significantly  improve your API’s efficiency and usability.
4:41
First of all, a well-designed API  also needs to evolve without breaking  
4:45
existing consumers. This is  where versioning comes in.
4:48
Imagine you've been running your e-commerce  API for years, but now your product data  
4:53
structure needs to change. Instead of  forcing all clients to update at once,  
4:57
your API architecture should be  flexible enough to support versioning.
5:01
The simplest approach is URL versioning, where  you include the version number in the endpoint,  
5:05
but header-based versioning, which keeps  URLs cleaner is also a viable option.
5:10
On top of that, giving clients  control over the amount of data  
5:13
transferred over the wire can do wonders  for performance. Fetching all products  
5:17
from the database might work fine when  you only have a few hundred entries,  
5:21
but things can rapidly deteriorate when  thousands of entities are involved.
5:24
Instead of returning everything  in one massive payload,  
5:27
pagination splits results into pages,  improving performance and usability.
5:31
Filtering and sorting through  query parameters is also essential,  
5:35
since users rarely just want  a list of all the products.
5:38
[4. Securing Your API] But all this work is useless  
5:40
if you are missing a key aspect of good API  design. A well-structured API is only valuable  
5:45
if it’s secure. Without proper safeguards, it  becomes an open door for unauthorized access,  
5:50
data breaches, and abuse. One of the most  effective ways to secure API endpoints  
5:55
is through authentication and authorization.  This is where JSON Web Tokens come into play.
6:00
These tokens are a compact and self-contained  way of securely transmitting information between  
6:04
parties. When a user logs in, the server generates  a token containing their identity and permissions,  
6:10
signs it with a secret key, and sends  it to the client. From that point on,  
6:14
every API request should include the  token in the Authorization header.
6:18
Since the token is signed, the server can  verify its authenticity without having to  
6:22
check a database on every request. This makes JWTs  efficient for stateless authentication. However,  
6:28
there are a couple of important  rules when working with JWTs.
6:32
First, set an expiration time to prevent  long-lived tokens from being exploited. Second,  
6:37
you should not store sensitive information  inside the token, as anyone with access  
6:41
to it can decode the payload. And finally,  all communication should happen over HTTPS.
6:47
By the way, EchoAPI makes securing your  API a breeze with support for multiple  
6:51
authentication methods so don’t forget  to check them out at the link below!
6:55
[5. Performance and Rate Limiting] It should not come as a surprise that  
6:57
your API is practically useless  if it does not scale under load.
7:00
One of the easiest ways to improve API performance  is caching. Instead of hitting the database for  
7:05
every request, frequently requested data can  be stored and reused. This can happen at three  
7:10
levels: on the client side, to prevent redundant  calls, at the CDN level to cache responses closer  
7:16
to the user or on the server where the API  itself caches database queries to reduce load.
7:21
But even the best-optimized API can  crash under excessive traffic. Rate  
7:25
limiting is a great mechanism  to prevent abuse. This restricts  
7:29
the number of requests a client can  make within a certain time window.
7:32
A common approach here is  the token bucket algorithm,  
7:35
where each client gets a limited number  of request "tokens" per minute. Once  
7:39
they’re used up, further requests  are blocked until the limit resets.
7:43
[6. Documentation] Another important aspect  
7:44
that is rarely discussed is API documentation.  Luckily for us, there are standardized tools and  
7:49
practices that make this process easier, ensuring  that APIs remain understandable and maintainable.
7:54
The OpenAPI Specification is considered  the industry standard for documenting  
7:59
RESTful APIs. It provides a structured way to  define endpoints, expected request parameters,  
8:05
response formats, authentication  methods, and error handling.
8:08
So remember that designing a secure,  scalable, and efficient API isn’t just  
8:13
about following best practices, but also about  understanding why these principles matter. 
8:17
If you enjoy this type of content  you should check some of my other  
8:20
videos. Until next time, thank you for watching.



Search in video
Introduction – REST APIs: Lightweight, Scalable, and Easy to Use
0:00
hey bite monks in my previous video I
0:02
explained why rest apis have become the
0:04
standard for building web services they
0:06
are lightweight scalable and easy to use
0:09
in this video I'll break down the top
0:11
and must know best practices when
0:13
designing and implementing rest apis
0:15
with Java examples so let's get
0:18
[Music]
HTTP Methods – Correct Usage for Actions Like GET, POST, PUT, PATCH, DELETE
0:22
started first of all use HTTP methods
0:25
correctly now I have already explained
0:28
and covered them in depth in my last
0:29
video in short think of these as the
0:31
verbs of your API they Define the
0:34
actions you're performing so you must
0:35
adhere to the semantic meaning of HTTP
0:38
methods get is to retrieve data and
0:41
should be always itemp poent meaning
0:43
multiple identical request should have
0:45
the same effect as one post is to create
0:48
a new resource and is not item poent put
0:51
is to update an existing resource
0:53
entirely which is always item poent
0:56
patch is to partially update an existing
0:58
resource which is not NE item poent and
1:01
delete is to delete a resource and it's
1:03
always item poent because if you delete
1:05
the same resource 10 times it's going to
1:07
have the same
1:08
effect next let's talk about how we
Resource Design – Using Nouns and Hierarchical URIs Effectively
1:11
structure our URLs or uis to be precise
1:14
in rest apis we use Uris to identify
1:17
resources the key thing to remember is
1:20
to focus on things not actions so
1:23
instead of describing what you are doing
1:25
you described what you're working with
1:28
so you must avoid actions in uis and
1:31
focus on resources and use hierarchical
1:33
uis for nested resources the key here is
1:36
to use nouns for your resources and
1:38
avoid putting actions directly into the
1:41
URLs think about it this way the URL
1:44
should represent what you are working
1:46
with and the HTTP method tells you what
1:48
you're doing with it so for example SL
1:52
customers is good that represents the
1:54
resource customers then you use get to
1:58
retrieve customers post to create new
2:00
ones put or patch to update them and
2:03
delete to well delete them on the other
2:06
hand URLs like/ get customers or/ create
2:10
users are bad practice they are
2:12
redundant because the HTTP method
2:14
already tells us what action is being
2:16
performed also if you have nested
2:19
resources you use hierarchical uis for
2:22
example SL customers SL1 23/ orders to
2:26
get the orders for a specific
2:28
customer now let's talk about status
HTTP Status Codes – Communicating Outcomes with Standards
2:31
Cotes these are super important for
2:33
communicating the outcome of a request
2:35
to the client in Spring response entity
2:37
class controls the whole response
2:39
response entity. created is 2011 created
2:42
response entity. okay is 200 okay and
2:45
response entity. not found is 404 not
2:48
found it's how we build correct rest
2:51
responses and here are some of the most
2:53
common ones 200 okay is for Success when
2:57
everything goes smoothly 2011 created is
2:59
is when a new resource was successfully
3:01
created 204 no content is when the
3:04
request was successful but there was no
3:06
content to return in the response 400
3:09
bad request means the client sent
3:11
something that the server couldn't
3:13
understand like invalid input 401
3:16
unauthorized is for authentication is
3:18
required to access this resource 403
3:21
Forbidden is when the client is
3:22
authenticated but doesn't have any
3:24
permission to access the resource 404
3:27
not found is for the requested resource
3:29
could not be found and 500 is when
3:32
something went wrong on the server
3:34
site moving on to error handling that is
Error Handling – Descriptive and Consistent Responses in Spring
3:37
when things do go wrong it is crucial to
3:40
provide clear and descriptive error
3:42
messages in a consistent format and this
3:44
helps our clients to understand what
3:46
happened and how to fix it this code
3:49
here shows how to do it globally in
3:51
Spring using add rest controller advice
3:54
we can catch resource not found
3:56
exceptions so instead of letting the
3:58
application crash or sending back
4:00
cryptic messages we create a clear error
4:02
response with a 404 not found status
4:05
this gives clients the information they
4:07
need to understand what went wrong and
4:09
how to potentially fix it so good error
4:11
handling makes your API more
4:13
userfriendly and
4:15
reliable we also want to validate
Validation – Using Hibernate Validator for Clean Input
4:17
request that is using Frameworks like
4:19
hibernate validator in Java to make sure
4:21
the data coming in is what we expect
4:24
this Cod snippet here handles creating
4:26
new users in our spring API the ad post
4:29
mapping annotation defines this method
4:31
as a Handler for post request sent to
4:34
the/ users
4:35
endpoint the add valid and add request
4:38
body annotations ensure the incoming
4:40
user data is valid and comes from the
4:42
request
4:43
body the code also calls user service.
4:47
saave User it's likely persisting the
4:49
user data and if successful it
4:52
constructs a response entity which sets
4:54
the status code to http status. created
4:57
informing the client that a new user was
5:00
created next up is versioning as your
API Versioning – URI vs Header Versioning for Backward Compatibility
5:03
API grows and changes you will need to
5:05
update it but how do you do that without
5:07
breaking existing apps that rely on the
5:09
old version the answer is versioning it
5:12
lets you introduce new features or
5:14
changes while keeping older versions
5:16
available and there are two main ways to
5:19
do this Ur versioning like slv1
5:22
customers and slv2 customers or header
5:25
versioning using a customer header like
5:27
accept version V1
5:30
let's first talk about URI versioning
5:32
this is the simpler approach you just
5:34
include the version number directly in
5:36
the URL for example slv1 here would
5:39
access version one of the customer API
5:41
while slv2 would access version two see
5:45
how we have two separate controllers one
5:47
for each version this keep things nicely
5:50
separated the second approach is header
5:52
versioning here the version is specified
5:55
in custom HTTP header like accept
5:57
version for example a client would send
6:00
a request with a header accept version
6:02
V1 this is a little more complex to
6:05
implement but it keeps your URLs cleaner
6:07
and here is an example with spring web
6:09
flux here both methods have the same URL
6:13
but are differentiated by the accept
6:15
version header this approach is more
6:17
flexible but requires a more complex
6:18
setup now when your API deals with tons
Pagination, Filtering, Sorting – Managing Large Data Sets Efficiently
6:21
of data sending it all at once can be a
6:23
nightmare that's where pagination
6:25
filtering and sorting come in pagination
6:28
breaks the data into smaller manageable
6:30
chunks like pages in a book filtering
6:33
lets clients narrow down the results to
6:35
only what they need and sorting lets
6:38
them organize the data in a specific
6:41
order let's start with pagination
6:43
imagine you have a millions of users you
6:46
wouldn't want to send all of them in a
6:48
single response instead you would send
6:51
them in Pages here is a simplified
6:53
spring example using
6:55
pable here the add request param
6:58
annotations let clients specify the page
7:00
number and size of each page spring data
7:03
jpa then handles the page ination for us
7:06
we get back a page object that contains
7:07
the current page of users as well as
7:09
metadata like total pages and total
7:12
elements now let's talk about filtering
7:15
suppose clients want to find users by
7:17
name we can use Query parameters for
7:20
this if the name parameter is provided
7:22
we use a custom query method to find
7:25
users whose name contain the given
7:26
string otherwise we return all users
7:30
finally sorting clients might also want
7:33
to sort users by their registration date
7:35
and again we can use Query parameters
7:38
here if the sort by parameter is
7:39
provided we use spring data sort to sort
7:42
the results accordingly a more advanced
HATEOAS – Enhancing Discoverability with Hypermedia Links
7:45
concept is hos or hyper media as the
7:48
engine of application State it's all
7:50
about including links in your API
7:52
responses that guide clients on what
7:54
actions they can take next it makes your
7:57
API more discoverable and easier to use
8:00
let's look at simple Java example using
8:02
spring Boot and spring H TOS to
8:04
illustrate how you might include these
8:06
hyper media links in your responses so
8:09
here in this score snippet we have a
8:11
simple book controller with two
8:13
endpoints the first one retrieves a
8:15
single book by its ID while the second
8:18
one retrieves all books notice that in
8:21
our getbook method rather than returning
8:24
just a book object we are returning an
8:26
entire entity model book this special
8:29
rapper from Spring Haws allows us to
8:31
attach links to our object so when a
8:33
client calls get API book SL1 a typical
8:37
Json response courtesy or spring hate to
8:39
us might look like this so what's
8:42
happening here entity model book wraps
8:45
the book resource to include links self
8:48
link points to the current book that is/
8:51
book1 and all books link allow the
8:53
client to discover the end point that
8:55
returns all books which is slash books
8:58
by embeding these links the the API
9:00
effectively guide clients on possible
9:01
next actions making your service more
9:04
intuitive and self-documenting finally
9:06
and very importantly security use https
Security –HTTPS, Authentication, and Authorization
9:10
for encrypted communication and
9:12
Implement authentication methods like or
9:15
2.o or JWT are common and use
9:18
authorization to control who has access
9:20
to what Spring Security is a popular
9:23
choice for this in Java and for in-depth
9:25
explanation of various API security
9:27
Concepts do check out my security
9:29
playlist in the description each video
9:32
focuses on a specific topic so you can
9:34
easily find what you're looking for and
9:36
I hope you'll find them interesting and
9:37
helpful as others have so those are some
9:40
of the key best practices for building
9:42
rest apis remember these are guidelines
9:45
and you should always adapt them to fit
9:46
your specific needs but following these
9:48
principles will help you create easy to
9:50
ous web services I'll see you in the
9:52
next video
9:55
[Music]
9:59
oh


Transcript for [How to Design APIs Like a Senior Engineer (REST, GraphQL, Auth, Security)](https://www.youtube.com/watch?v=7iHl71nt49o) by [Merlin AI](https://merlin.foyer.work/)

In this course, you'll learn the API design skills that separate junior developers from seniors. Most developers only know how to build basic CRUD APIs, but they don't really understand how APIs work behind the scenes, like when to choose REST over GraphQL, or when to choose rich protocol like HTTP, websockets, messaging protocols, or how to apply security practices. These are exactly the things that senior engineers get asked in interviews and the same principles I've applied myself while working on real world projects. We'll go through the API design principles, protocols, restful and craft API design, authentication, authorization and security practices. So everything you need to know to go beyond the basics and think like a senior engineer. If you're stuck in junior to mid-level roles and want to land senior salaries, then this is the knowledge that will help you get there. Welcome to this section where you will learn the fundamental principles of API design which will enable you to create efficient, scalable and also maintainable interfaces between software systems. Here is what we're going to cover in this lesson. We'll start from what APIs are and what is their role in system architecture. Then we'll cover the three most commonly used API styles which are REST, GraphQL, and gRPC. We'll discuss the four essential design principles that make great APIs and also how application protocols influence the API design decisions. We'll also cover the API design process. So starting from the design phase to development phase to deployment. So we'll see how that process looks like. So let's start by understanding what is an API. API stands for application programming interface which defines how software components should interact with each other. Let's say on one side you have the client which is either the mobile phone or the browser of this user and on the other side you have the server which will be responding to the requests. So API here is just a contract that defines these terms which are what requests can be made. So it provides us with an interface on how to make these requests meaning what endpoints do we have what methods can we use and so on. Also what responses can we expect from this server for a specific endpoint. So first of all it is an abstraction mechanism because it hides the implementation details while exposing the functionality. For example, we can make a request to save a user data in this server, but we don't care at all about how the logic applies behind the scenes inside of this server. So, we only care about the interface that is provided through this API and we only use that endpoint and we store the user without even knowing about the implementation details. And it also sets the service boundaries because it defines clear interfaces between systems and components. So this allows us to have multiple servers. We can have one server that is responsible for managing the users. We can have another one that is responsible for some other records. Let's say for managing the posts and so on. So this allows different systems to communicate regardless of their underlying implementation like client browsers with servers or servers with another servers and so on. Now let's focus on the most important API styles you will encounter during the design phase. These are RESTful, GraphQL, and gRPC. The most common one out of these is REST, which stands for representational state transfer. These type of APIs use resource-based approach by using the HTTP methods as a protocol. One of the advantages of REST APIs is that they are stateless, meaning that each request contains all of the information needed to process it and we don't need any prior requests to be able to process the current request. And it uses the standard methods on HTTP protocol which are get for fetching data, post for storing data, put or patch for updating data and delete for deleting data. So based on its characteristics, the rest is most commonly used in web and mobile applications. Next, we have GraphQL, which is the second most common API style after the REST APIs. GraphQL is a query language that allows clients to request exactly what they need. This means that it comes with a single endpoint for all of the operations and we can choose what we are expecting to receive from this API by providing the payload in the request. And the operations here are called query whenever we are retrieving data or mutation whenever we are updating data. So this is the equivalent in put or patch or post in the restful APIs and there is also a subscription in operations which is for realtime communication. The advantage of GraphQL APIs is that it allows us to have minimal round trips. Let's say we need some data that in restful APIs we will need to make three requests to get all of this data. In GraphQL case we can make a single request and get all of these data avoiding the unnecessary two requests that we will otherwise have to make in RESTful. And because of that this is the recommended option for complex UIS. So wherever you have some complex UIs where on one page you might need different data on another page you might need some other complex nested data. In these cases, GraphQL is the better choice over restful APIs. And the last option is gRPC. I would say this is the least common one out of these three. GRPC is a high performance RPC framework which is using protocol buffers for communication. The methods in gRPC are defined as RPCs in the protoiles and it supports streaming and birectional communication. This is an excellent approach for microservices especially and internal system communication as it is more efficient when you're working between servers compared to graphql or compared to restful APIs. So the difference between rest graphql and gRPC APIs is kind of clear but let's also clarify the real difference between rest and graphql APIs on examples. So as you saw rest comes with resource-based endpoints. For example, here if we take a look at these requests, you can see that the resource here is users. So you always expect to see some users endpoint or some followers endpoint or let's say posts endpoint. So it is resource-based and sometimes we might need to make multiple requests for getting the related data. As you can see here, we need let's say the user details, but we also need the user posts and followers. So in this case we need to make three requests to get all of these data and it uses HTTP methods to define operations. As you can see these are HTTP endpoints and we are using the get method specifically and the response structures are fixed meaning if you got one response for this specific user. Next time you can expect to have exactly the same response structure. Maybe some data will be modified but the structure always remains the same. And it also provides explicit versioning. So as you can see it comes with vub1 for the v1 API then later if it got a major upgrade then this will become v2 and so on. And you can use the headers on the requests to leverage the http caching on restful apis. Now if we compare that to graphql apis it comes with a single endpoint for all operations. So mostly it is /g graphql or slash some API endpoint that is commonly used for all operations and in this case we will use a single request to get the precise data that we need and we will use the query language of graphql. This is what the query language looks like. As you can see we start with a query and then we define what we need. For example, we need the user with ID 1 2 3. Then we need the name of the user, the posts and then we define whatever we need from the posts. Maybe we need only title and content and nothing more. And also the followers and what we need from followers, maybe only names. So this allows us to be more efficient in our requests compared to restful APIs where we will need to make free requests for this same data. This means that client needs to specify the response structure and in this case the schema evolution is without versioning. So here as you saw it is with v_sub_1, v2 and so on. In this case the schema usually evolves without versioning. But there is also a common pattern to start versioning the fields. For example you can have followers v2 and that will be the second type of followers schema. But you can also go without versioning. So you can just start modifying the followers or posts if you are sure that there are no other clients using your old API and in this case you can leverage the application level caching instead of the HTTP caching. Now let's discuss the major design principles that will allow us to create consistent, simple, secure and also performant APIs. Ultimately the best API is the one that we can use without even reading the documentation. For example, if you saw the previous endpoints in the users you see that we have / users/123 and obviously we are expecting to get the user details of this specific user. And if you make a request for example to that endpoint to fetch user details but then you find out that it also updates some followers or something while making this request then obviously that is a very bad type of API as we didn't expect it to do such operations. So first of all the good API should be consistent meaning it should use the consistent naming casing and patterns. For example, if you use camel case in one of the endpoints, let's say you have user details and you do this in camel case, but in another case you do it with a skinnate case like user/details, then this is not common and this is not consistent. The second key principle is to keep it very simple and focus on core use cases and intuitive design. So you should minimize complexity and aim for designs that developers can understand quickly without even maybe reading the documentation. And simplicity again comes down to this which is the best API is one that developers can use without even reading the documentation. Next obviously it has to be secure. So you have to have some sort of authentication and authorization between users. Also, if you have inputs, then you need to make sure that these are validated and you should also apply rate limiting. So, these are the most basic things that you have to do to keep your APIs secure. And the last pillar is performance. So, you should design for efficiency with appropriate caching strategies with pagination. If you have a large amount of data, let's say thousands of posts, you don't want to retrieve all of these whenever they make a request to get the post. So you should always have pagination with some limit and offset. Also the payloads meaning the data that you will send back should be minimized and also whenever possible you should reduce the round trips. So if you have the opportunity to send some small data along with the request of one of the endpoints then it's better to do this if you know that you're going to use it instead of making another endpoint for making a request to get the same data. Now each of these APIs use different protocols and we will learn more about these in the next lesson. But basically your protocol choice will fundamentally shape your API design options. For example, the features of HTTP protocol directly enable restful capabilities. So it makes more sense to use HTTP along with restful APIs because it also provides you with status codes and these are great to be used with crowd operations that you will have in restful APIs. On the other hand, web sockets which is another type of protocol enable realtime data and also enable birectional APIs. So this can be used along with realtime APIs wherever you need some chat application or some video streaming. This is a good use case of websocket APIs. In case of graphql APIs, you again will use the HTTP protocol instead of websockets or gRPC. GRPC on the other hand can be used along with microservices in your architecture to make it faster compared to HTTP. So your protocol choice will affect the API structure and also the performance and capabilities. Therefore, you should choose it based on its limitations and strengths and the one that makes more sense in the type of API that you'll be developing. Now, let's discuss the API design process. It all starts with understanding the requirements, which is identifying core use cases and user stories that you will need to develop. also defining the scope and boundaries because if it's a huge API then you probably won't develop all of the features at once. So you should scope it to some specific features that you'll be developing and also what are out of scope for now. Then you should determine the performance requirements and specifically in your API case what will be the bottlenecks and where you need to make sure that it's performant and you should also not overlook the security constraints. So you should implement all of the basic features like authentication, authorization, the rate limiting but maybe some more stuff depending on the API that you'll develop. When it comes to design approaches, there are couple of ways to go about it. The first one is top-down approach which is you start with highle requirements and workflows. This is more common in interviews where they give you the requirements on what the API will be about and then you start defining what the endpoints will be, what the operations will be and so on. But there is also the bottom up approach which is if you have existing data models and capabilities then you should design the API based on this. So this is more common when you're working in a company and they already have their data models and capabilities of their APIs. So you should take that into account when designing the API. And we also have contract first approach which is you define the API contract before implementation meaning what the requests should look like and what the responses should look like. And this is more similar to top-down approach and this is also commonly used in interviews. When it comes to life cycle management of APIs, it starts with the design phase where you design the API, discuss the requirements and the expected outcomes of the API and only after that you can start the development and maybe local testing of your API. After that you usually deploy and monitor it. So you do some more testing but now on staging or on production. But then it also comes the maintenance phase. And this is why it's important to develop it with keeping the simplicity in place. So it will be easier for you to maintain or for other developers to maintain in the future. And lastly, APIs also go through deprecation and retirement phase. So some APIs eventually get deprecated because there might come up with a new version of the API that you should use. Or let's say you are transitioning from v1 to v2 API. So that's also the deprecation phase of the v1 API. So developing APIs is not only in the development phase as you might assume. It's not just coding. So the big part of it is designing it and also keeping it maintainable and also eventually you might need to retire it at the end. So let's recap and see what our next steps are. We learned what APIs are and about the most dominant free type of API styles which are restful, GraphQL, and gRPC. We've covered the four key principles that will guide us when creating API designs effectively. And you now also understand how the design choice of your protocol will influence the design of your API and also the whole API design process from start to finish. But we didn't discuss the limitations and strengths of these API protocols. So that's why in the next lesson we will learn all about the API protocols that we can use with API design and which one we should choose based on the requirements of our API. Before we get into the next lesson, just knowing the API principles on a high level won't get you far. In interviews, you'll usually get code immediately if you're trying to fake it and if you have never implemented them in real projects. If you're a developer with 1 to five years of commercial experience based in US, Canada, Europe, Australia, or New Zealand, and you're stuck in junior to mid-level roles, but you want to move into senior positions, master API design and other concepts and start earning senior level salaries, then you can apply to work with me oneonone. This is exactly what I help developers do inside of my mentorship program. We take these concepts and apply them hands-on in real world projects. The same way senior engineers will work in top companies. So the first link in description is where you can apply. But only apply if you're serious about advancing to senior roles. Otherwise, our calendar is fully booked all the time. And if your application doesn't seem like a good fit, then unfortunately we'll have to cancel it. Choosing the wrong protocol for our API can lead to performance bottlenecks and also limitations in functionality. That's why we need to first understand these protocols which will allow us to build APIs that meet our specific user requirements for latency throughput and also interaction patterns. That's why in this lesson we'll cover the role of API protocols in the network stack. the two fundamental protocols which are HTTP and HTTPS and also their relationship to APIs. Also another common type of protocol which is websocket for realtime communication we'll also cover advanced message queuing protocol which is commonly used for asynchronous communication and lastly we'll cover the gRPC which is Google's remote procedure call and it is also another common type of protocol used commonly within servers. Let's start by understanding the application protocols in network stack. Application layer protocols sit at the top of network stack building on top of protocols like TCP and UDP which are at the transport layer. These protocols at application layer define the message formats and structures also the request response patterns and management of the connections and error handling. Now below that we have many other layers like the network layer or data link layer or even physical layers but when building APIs we are mostly concerned with the API layer protocols which are HTTP, HTTPS, websockets and so on. The most common type of protocol and also the foundation of web APIs is HTTP which stands for hypertext transfer protocol. This is the typical interaction between client and server when they are interacting over HTTP. As you can see, client always sends an request and they define the method which can be get, post or other methods and they define the resource URL which can be at / API/ products. Let's say they are requesting data for this specific ID of the product and they also define the version of the HTTP protocol that they are using. They also define the host which is the domain of your server where the information is accessed and usually they also authenticate before accessing any resources. So it can be either a bearer token or a basic authentication of and so on. So once the request is authenticated in the server it receives the response which is in similar format and it's in HTTP response. So you get the HTTP version which is again the same as you requested with and the status code which can be 200 if it was successful or it can be 400 if the client was error or 500 if the error happened in server and so on. You receive the content type which can be usually application JSON but it can also be a static web page or something else. And there are many other headers that you can control like controlling cache. You can use the cache control header or some other properties. But these are the main things that you would notice in HTTP request response cycles. Now when it comes to methods, you have get for retrieving data, post for creating data in the server, put or patch for updating data partially or fully, and delete for removing data from the server. And when it comes to status codes which are received by the server, so you have 200 series which are successful cases. You have 300 for redirection. 400 means that client made an error in the request. So this is an issue from client side or 500 which means that server made an error or like some error happened in the server. So which means that this is the issue in this server. And these are the common headers like content type which is defined by the server usually but also from the client authorization for making a request and authorizing to the server. Accept headers cache control user agent and there are more headers but these are the common ones. Then we also have HTTPS which is basically the same HTTP protocol but with some sort of TLS or SSL encryption which means that our data is now protected in transit when we are making requests. So it adds a security layer through this TLS or SSL certificates and encryption and it protects data in the transit and benefits of HTTPS is obviously your data is encrypted in the transit. It comes with data integrity and you also authenticate users before providing any data and it also adds SEO benefits and you have many risks when you are using HTTP only without any encryption. So the golden standard is to always use HTTPS in servers. The next type of protocols are web sockets. While we have HTTP which is very good at request response patterns, sometimes HTTP has limitations. For example, let's say you're pulling some data. Let's say this is a user chat. So you have the client and server. On the client side, you have the user chat and on the server you have the messages between two users. When one of the users messages the other, it sends a request to the server to notify that a message has been sent. And it receives a response from the server, maybe the messages from the other users if there are any. And then next time if you need to know if you have new messages, you need to make again another request to the server and maybe you don't have any new messages. So you will receive an empty response with no new data. So this was basically an unnecessary request response cycle and you might request from some other time let's say from 1 minute and receive a response. Now you have some messages but it can be also empty again. So this way is not ideal for realtime communication. As you can see, you get increased latency. You waste some bandwidth with making requests that are empty and you also use the server resources without the need of making requests to this server. And for such cases, we have websockets which solve this issue. So in websocket you have usually a handshake that is happening within the first request and now you have both like two-side communication between client and the server which means that once the handshake is been made the server can independently decide to push data to the client. Let's say now you have two new messages on the server. So server can decide to send these messages to the client without even client requesting for it. But client can still request data. So if client needs some external data or more data from the server, it can still make requests. But server is now also able to independently push data to the client. So this is what unlocks the real-time data with minimal latency. As soon as you have some new data in the server, it pushes the new data to the client and it also reduces the bandwidth usage by allowing birectional communication. In client server model with HTTP you would make let's say new requests per 5 seconds or 10 seconds to see if there are any new data in the server. But in this scenario you don't make any more requests other than the first one. And now whenever there are new data server will push it and whenever there are no data to be requested then you don't need to make unnecessary requests to the server. The next very common type of protocol is advanced message queuing protocol which is an enterprise messaging protocol used for message queuing and guaranteeing delivery. In this setup you usually have the producer which can be either a web service or payment system or something like that and on the other side you have the consumer which can be the processor of the payments or notification systems and stuff like that. So producer publishes messages to the message broker and here is where you have the advanced message queuing protocol. You have cues in the middle. Let's say one of these cues is for order processing. So whenever a new order has been placed, producer publishes a message to this queue. And then whenever this consumer is free, it can pull messages from this queue and start updating the inventory and data in the database. This allows the consumer to only pull data from here whenever it has capacity. And whenever this consumer is busy with some other tasks, it leaves the message in the queue. And then later on, whenever it has some free capacity, it will pull the message and start updating the data. And when it comes to exchange types, you have direct one-on-one exchange or fan out or topic based communication. And we will explore these more when we come to the message queuing section. The other common type of protocol is gRPC which works with protocol buffers. This is a high performance RPC framework invented by Google and it uses HTTP2 for transport meaning the second version of the HTTP. This means that clients should support HTTP2 otherwise this can't be used between client and server but that's why this is most commonly used between servers. So usually the client is another server and we have some other microservices communicating with each other with this gRPC framework. It mainly uses protocol buffers and it also comes with built-in streaming capacities because it uses HTTP.2. So these are the most common types of API protocols. There are many more but usually in 90% of cases you would see only these protocols. And when choosing the right one, you should mainly consider the interaction patterns. Usually, by default, you go with HTTP. If it's just a request response cycle, but if you're building something like real-time chat or some real-time communication, then you would need to go with websockets. The choice also depends from the performance requirements. So if you have multiple servers, microservices communicating with each other and there isn't opportunity to use gRPC for example then you can go with it to increase the performance and speed of the communication but it also comes down to client compatibility. For example, most browsers don't support the latest version of the HTTP. That's why gRPC isn't that very common for browser server communication. It also comes down to the payload size meaning the volume of the data and encoding security needs based on the authentication encryption and so on and also the developer experience. So the tooling and documentation and it also comes down to the developer experience because you're mostly going to work with this API and it needs to have good documentation and tooling for you to fully work with this type of API protocol. So to recap, we have explored the role of application protocols in network stock. The HTTP and HTTPS which are the most fundamental types of protocols. Web sockets for real-time communication. AMQP which stands for advanced message queuing protocol which allows us to have asynchronous communication and adding message cues between the consumer and producer and also gRPC which stands for Google remote procedure call. And the main advantage of this is that it's high performance RPC framework which uses HTTP2 for transport. So we discussed the application layer which includes these protocols that we usually use for building APIs. But we don't know yet about this transport layer which includes the TCP and UDP. So in the next lesson we are going to discuss this layer and understand which of these transport layers whether TCP or UDP are the best choice depending on the API that we are building. Most developers work with APIs but never think about what's actually delivering those packets. Like how does it happen that the request is being made from client to server and how does this request go through the internet. That's where the second layer comes in in the OSI model which is the transport layer that has the TCP and UDP inside of it. These are both transport layer protocols, meaning they handle how data moves from one machine to another over the network, but both are doing it very differently. In this lesson, we'll learn about these transport layer protocols. We'll start with TCP, which is the reliable but slower version. Then we'll learn about the UDP, which is in short, it's faster and unreliable version of TCP. and we'll compare both of them and decide which one we need to choose based on the API requirements. Let's start with TCP which stands for transmission control protocol. Think of it like sending a packet with a receipt tracking and also signature that is required. So when you send some packets over the internet, you usually don't send all of it at once. Sometimes the data is larger. Let's say it's divided in three chunks. So you need to send them separately. the first chunk, the second chunk, and also the third chunk. So in this case, TCP guarantees delivery of all of these three chunks. If one of these packets is lost or arrives out of order, TCP will resend or reorder it. It's also connection based, which means that before sending any data, it performs a free-way handshake, which is establishing the connection between client and server. It also orders these packets. Let's say the client receives the first packet first, then the third packet, then the second packet. It makes sure that it's reordered to first, second, and third. This of course adds overhead, but it ensures that it's accurate and reliable. That's why APIs that involve payments, authentication or user data always use TCP. On the other hand, we have UDP, which stands for user datagram protocol. It's fast and efficient, but the downside of this is that it doesn't guarantee that all of the packets will arrive. For example, if you're sending four packets from the server to the client, one of these packets might be lost and it won't be pushed to the client and UDP won't make sure that this eventually gets delivered. So, there is no delivery guarantee. There is also no handshake or connection or any sort of tracking. But because of these trade-offs, it is faster transmission and it comes with less overhead as it doesn't need to make sure that all of the packets are delivered or in the correct order. For example, in video calls, UDP can be the best protocol because if some information was cut in the middle or let's say you're in a call with someone and their internet connection lacks, you don't need to receive that old connection or the old data on what they said because you are in the call right now. So UDP is the go-to for video calls, online games, or live streams because if one of these packets drops, it's still fine and you don't need to go back and resend this packet. You can just move on and send the next packets. This is what the three-step handshake looks like in TCP. As you can see, the first step is that client sends a request to the server. In the second step, server syncs and acknowledges the request. And in the first step, the client acknowledges the server and this is where the connection is established between the client and server. And now they can start sending data back and forth on top of this TCP protocol. So in short, TCP is the safer and reliable version of UDP, but it is slower. And on the other hand, UDP is faster and lightweight, but it is risky. For example, if one of the packets in between the source and destination is lost, it doesn't resend it. So there is no guaranteed delivery. But on the other hand, if in TCP one of the packets is lost after some time out, it still resends the first packets. And this way it guarantees that all data will be delivered compared to UDP where some data might be lost, but it will still keep going. And when choosing between those two, these are the main things that you need to look for. If you need the connection to be safe and reliable, then you need to go with TCP. Or if you need it to be fast, lightweight, but some data loss might be acceptable, then you will need to go with UDP. For example, it is best for using TCP in bankings, emails, payments, and so on. And on the other hand, UDP is mostly used in video streaming, streaming, gaming, and so on. These are the main things that you need to know about the application and transport layers. And these are the only layers that will need to be used to building APIs. And in the next lesson, we will learn about restful APIs and how we usually design APIs in restful format. Restful APIs let different parts of a system talk to each other using the standard HTTP methods. They are the most common way developers build and consume APIs today. And in this video, you'll learn how to design clean REST APIs by following the proven best practices so that you avoid creating messy and inconsistent patterns that make the APIs hard to use and maintain. We'll start by learning about the architectural principles and constraints of restful APIs, about the resource modeling and URL design, also the status codes and the error handling as well as filtering, sorting, and so on. and we'll learn the best practices when using and developing restful APIs. Let's start from the resource modeling. Resources are the core concepts in REST. Let's say you have the business domain which consists of the products, orders and reviews. When modeling this to a restful API, you usually convert this into nouns and not verbs. Meaning that the product becomes products, order becomes orders, and same for the reviews. These can be collections or individual items. For example, this first request which is to / API/ products will return you the collection of products, not a single product. But on the other hand, you could have slash products and slashsp specific ID of a product which will return you the individual item. And notice that we are using / products when retrieving the collection of products. And we are not using something like get products which will be not a best practice in restful APIs. As I mentioned we are using nouns here and not verbs. So to fetch orders for example you don't define the URL as get orders. You just define it as slash orders and depending on the method that we'll use let's say it's a get method then you will retrieve the orders. If it's a post method then you will create an order and so on. So all the resources should be clearly identifiable through the URLs. For instance, this is an example of getting a collection. This is an example of getting a specific item. And also nested resources should be clear defined. For example, if you want to retrieve reviews for some specific product, then we would assume that if you make a request to SL products/ ID of that product and then / reviews, you would get the reviews for that specific product. But in real world APIs, you rarely want to return all the results at once. That's why we usually incorporate filtering, sorting, and pagionation in APIs. So, let's start from the filtering. For example, if you make a request to get all the products, you usually add some query parameter, which in this case, you can see it's category. So, you're first of all filtering them by category. And then also with the end sign, you add that they should be in stock. So, the in stock should be true. And this way, you are only returning the items that you're going to display on the UI. And you're not making some requests that will waste the bandwidth of this API. and also it will be a huge response for you in the front end side. Next we also have sorting. In this case again it's controlled through the query parameters and query parameters are anything that start after the question mark in the URL. So in this case you usually pass the sort attribute and this can be for example ascending by price or ascending by reviews or it can be also the descending order. So based on this you will get the response from the API in a sorted order because if you for example have thousand items in the back end in the database you don't want to retrieve all of these in unsorted order to the front end because let's say the front end now needs to sort them by the price ascending. This means that it needs to make request to get all of the products which are these thousand items that you have in the database. So that will be very inefficient. That's why we do the sorting in the back end instead. So your back end should support sorting functionality. This way the front end can just make a request to your back end and pass this sort query parameter and then that way it will get the sorted products to be displayed on the screen. And next we also have pagination. Again with the query parameter you usually pass the page which you want to retrieve and also the limit because if you don't pass the limit then again it will give you all of the products starting from the page two till the end which can be a lot of items. So you also pass some sort of limit and that limit is whatever you're going to display on the front end and then based on that you will get the response and here let's say you fetched 10 items so you're going to display those 10 on the UI and then once they click on the next page you will make another request to the page three this time and you will get the next items from the server. Now usually we use page for pagination but there is another common attribute that is offset. So some APIs use offset instead of the page and they use this in combination with limit which basically means if you have thousand items. So offset will tell the API from where to start counting this thousand items and then limit is the same as you have it here. So it's basically limiting the number of items that you are getting from this offset to retrieve to the front end. And the last option you can also have this cursor based. So instead of page and limit you would pass a cursor which will be the hash of the page you want to retrieve. So this approach of adding filtering sorting and pagination comes with benefits. So first of all it saves the bandwidth of your server. It also improves the performance both in the server side and on the front end side. And it also gives the front end more flexibility because now you can fetch only the things that you need and not some unnecessary data from the database. Now let's come to the HTTP methods that REST APIs use because they rely on HTTP protocols and hence they are using the HTTP methods especially for crowd operations. So these are the most common types of crowd operations you would see in REST APIs. First of all we have the get method which is used for reading data from the API. So this is for retrieving resources as you saw like retrieving the products, retrieving the reviews and so on. And the URL usually looks like this. You you make a get request to the / API/ version of the API/ the resource name. And these type of requests are both safe and item ponent. Which basically means if you make a request to slash products two or three times, you expect to receive the exact same output every time unless some new products obviously have been added to the database. Next, we have the post method. This is usually when you're creating a resource in your server. The common example is again you will make the request to exact same endpoint as you have it for the get to create a collection but in this case instead of get you are using post method and this tells the API that you need to create a resource in the products and not retrieve them. These type of requests change the state of the server. They are adding a new item and also they are not item ponent which means that they are creating a resource. So the first time you create a resource, you will get the ID of the first item that you created. The second time you create it, you will get the ID of the second one and so on. Next, we have the put and patch methods which are very similar, but they are updating resources in your API, but they do it a bit differently. The put method replaces the whole resource, whereas the patch method partially updates the resource in your API. Now you can see that the request URL is exactly the same in both of their cases. So it's to slash products slash id of a product you want to modify just in case of the put request it will take this whole product with the ID of 1 2 3 and it will basically replace it with the new one that is coming from the front end. Whereas in case of the patch it will again take this item from the database with ID 1 2 3 but it will update it partially. Let's say you just updated the title from the front end and you made the request it patch method. So this will only update the title of this product and it will leave the other parts other properties unchanged. And the last crowd operation is delete and we use delete method in this case and obviously as the name tells it deletes the resource from the database. So again the URL is exactly the same as you have for modifying items. it's to /roucts/ id of the resource and in this case you are not passing anything in the request body. So you are just making a delete request to this item and you are removing this from the database and each of these operations return you different status codes depending on how the request went whether it was successful or not. For that we have status codes and error handling in restful APIs. So you should use the appropriate status codes when working with REST APIs. For example, the 200 series are for successful requests. For example, 200 is okay. 200 is resource has been created. 204 is there is no content here. Let's say you made a request the previous request we were talking about to /roucts/ some ID of a product and you successfully retrieved this item. This means that you also need to set the status code to 200 because the request has been successful. In the other case where you're creating a product and you're making a post request to / products, this time you shouldn't response with the same 200 code because 200 generally means that the status was okay. But in 2011 case, it means that the resource has been created. And in this case, since you're creating a new product, you should obviously response with the 2011 status code, meaning resource has been created. We also have 300 series which are for redirection. Let's say you make a request to a URL and now this URL has been moved to somewhere else. So it will respond with a 300 series and it will redirect you to the new URL. In 400 series, we have the client errors. So this is whenever your front end made a bad request or the user made a bad request. For example, 400 is a generic bad request. In 401 we have unauthorized requests, meaning the user is not authenticated to make this request. For 404 we have not found. So generally when you visit some URL or you make a request for some specific resource that doesn't exist, you would get this 404 status code. So for 400 case, let's say you made a request with invalid parameters or some wrong JSON format. In this case, you would get a generic 400 repair request. But if a user makes a request to to get some product which is let's say the product with this ID and it doesn't exist in the database after querying it, then you should respond with the 404 status code, meaning that the resource has not been found. And lastly, we have 500 series. These are things when error happens in your server. So you don't know the exact reason and it's also not a client error meaning client requested everything properly. And in this case we throw unexpected server side errors. You generally respond with a server error message and you return the 500 status code along with it. When it comes to best practices of restful APIs, first of all notice that we are using plural nouns for all of the resources. So instead of slashroduct we are using /roducts for retrieving the products collection. So you should always use the plural in this case. Also in the crowd operations we use the proper HTTP methods. For example when making a request to delete users we expect to make a request to users/ ID of a user and not some post request to/ users/ ID. So first of all the HTTP methods needs to be properly set up and also the URL. We don't expect some random things like /dee to delete a resource from the database. As you saw we also support filtering sorting and pagination in good rest APIs. Not only pagination for example in this case we only have the page free but we cannot limit the amount of products that we want to retrieve. Whereas in this case we can fully control what we want to get from the API. We want to get the items from page three. We want this number of limit to be applied on the products. And we also want to apply some sort like sorting to sort the price or sort by ratings and so on. And also versionings in the restful APIs. As you noticed in all of these requests, they all come with a prefix which is / API and then slash the ID of the API which is either v_sub_1, v2, v3 and so on. Let's say in the future you migrate your API and you start using bunch of new features but you also break something in the previous version one then if you use the versioning you won't break it on the front end because they can use the old version of your API and still use the old features and functionalities while you continue to develop the new version let's say version three and you support new features here and you might have broken something here but they are still using the old API so this doesn't impact the end users. So to recap, we learned about the rest architectural principles and constraints. Also about the resource modeling and URL design and how we model the business domain into the rest to full API domain. Also the status codes, error handling and the proper methods to be used with the basic crowd operations. And lastly, we covered the best practices for restful APIs that you should use to keep your APIs consistent and also predictable for other developers who are using it. Before we move on to the next section, just knowing these crowd operations and routes, it's good as a starting point. But if you've never built a restful API or graphqle API at the lower level and implemented these concepts, then this is not going to take you far. You need to also do the practice other than the theory. If you're a developer with 1 to five years of commercial experience based in US, Canada, Europe, Australia or New Zealand and you're stuck in junior to mid-level roles, but you want to move into senior positions, master API design and other concepts and start earning senior level salaries, then you can apply to work with me oneonone. This is exactly what I help developers do inside of my mentorship program. We take these concepts and apply them hands-on in real world projects. The same way senior engineers will work in top companies. So the first link in description is where you can apply. But only apply if you're serious about advancing to senior roles. Otherwise, our calendar is fully booked all the time. And if your application doesn't seem like a good fit, then unfortunately we'll have to cancel it. Traditional restful APIs often return too much or too little data which requires us to do multiple requests for a single view to get all the data that we need. GraphQL solves this issue by giving clients exactly what they requested for. But designing GraphQL APIs is different from designing restful APIs. That's why in this video we'll cover the core concepts of GraphQL and why it exists. the schema design and type system of GraphQL, queries and mutations, error handling, and also best practices for designing GraphQL APIs. Let's start by understanding why GraphQL exists in the first place. It was created by Facebook to solve a very specific pain, which is clients needing to make multiple API calls and still not getting the exact data that they needed. For example, if you imagine we have the Facebook APIs like user API, posts API, comments and likes for the Facebook page. Most of the times client can make requests to all of these APIs separately and still not get all the data that it needs which will require it to do multiple requests to the same API. This of course adds up to the overall latency of the page because the page is still not loaded until all of these requests are made and the data is fetched. But in case of GraphQL APIs, you have a single GraphQL endpoint. So the client specifies the shape of the response and this one endpoint handles all of the data interactions. It is still an HTTP request, but as you can see, we can specify the exact data that we need. For example, we need the user with ID 1 2 3 and we need only the name of the user also posts and from the posts we can specify only title. So we don't need the images for this view. And again with the comments you can specify the exact data that you need within the object so that you are not doing overfetching of the data. Now let's see the schema design and type system of GraphQL and how it's different from restful APIs. The schema in this case is a contract between the client and server. In schema, first of all, you have types which can be for example user type that you specify and you specify all the fields that exist on this user type which are ID, name, posts and so on. And as you can see if the type is not a primitive type like posts then you can specify another type of post array and then this post type can be defined separately. Next we have queries to read data. So this is the equivalent of doing get requests in restful API. You specify the query and the function of this query. This can be the user query which fetches the user with specific ID and also the return type of this query which in this case is the user type that we defined above. And GraphQL also come with mutations. You can think of this as the equivalent to post, put, patch and delete methods in restful APIs. So anytime you are mutating a data in the database, you are making a mutation query. Here as you can see we have an example of create user method which accepts name and of course many things in real world and then it returns the user type that we have defined above. So if you have good schema design in GraphQL, it should mirror your domain model and it should be intuitive and flexible. Next, once you defined the schema design and type system, you can start querying and mutating data with this GraphQL API. For that, we have queries for fetching data. Again, this is like the get requests in restful APIs. And here you can specify exactly what you need from the user. This is the same user method that we defined there in the schema. So here you can also specify the exact attributes like the name posts and from posts you need the title only and this will make a request to your graphql API and return the exact data that you requested. Similarly you can also use the mutations that you defined. For example, if you have a create post method defined as a mutation, you can use this to mutate the post. for example, setting the title and body of the post and then you also specify what data you need to retrieve after this post is created which is ID and title. When it comes to error handling in GraphQL APIs, this is a bit different than in restful APIs since GraphQL always returns 200 okay status for all responses even if there was an error. In this case, we have to return errors field in the response which will indicate that there was an error. So partial data can still be returned with errors like in this case we have the user which is null and then we have the errors field which indicates that you have the status code 404 message not found and path which is the user in your schema. As you can see in this case you can specify the status code in the errors array. Since we are returning 200 status codes for all GraphQL requests, that's why we have the status code specifically mentioned in the errors so that we know what kind of error this is, which is user not found. There are also best practices that we normally follow when designing GraphQL APIs. First of all, the schemas that we saw, it's a good practice to keep them small and modular. Also, we should avoid deeply nested queries. For example, you can have a user and then nested post and then within the post you can have a comment. So this can be infinitely nested and to avoid that we usually implement query limit depths which is how deep you can go like how many layers nested you can have in your data. So you specify something like six or seven layers deep. We also use meaningful naming for types and fields so that it also makes from the client side because they both are going to use the same schema. And when mutating data, we always use the input types for mutations. Before a system can authorize or restrict anything, it first needs to know the identity of the requesttor. That's what authentication does. It verifies that the person or system trying to access your app is legit. And in this video, you'll learn how modern applications handle authentication from basic to bear tokens to OF2 authentication and GVT tokens as well as access and refresh tokens and also single sign on and identity protocols. Before learning the different types, let's first understand what is authentication. Authentication basically answers who the user is and if they are allowed to access your system. So whenever a login request is sent either by the user or another service this is where we confirm the identity of the user and either provide them access so approve their request or reject it with unauthorized request. This is basically the first step before authorization begins which is the topic of the next lesson. So before you access any data or perform any actions on this service, the system needs to know who you are and this is where the authentication is used. The first and simplest type of authentication is basic authentication. This is where you use username and password in combination and you send a login request which contains the base 64 encoded version of username and password. This is a very simple way of encoding data and it's easily reversible. And because it's easily reversible, it's now considered insecure unless it's wrapped within HTTPS. But even with that, it is now very rarely used outside of the internal tools in the company. Next, we have bearer tokens which are more secure compared to basic authentication. Here you send the access token with each request instead of the username and password encoding. So whenever the client needs to access resources, they send this token within the request and then your API verifies or rejects the token and if it verifies then you send the successful response with the data that they requested. Bear tokens are the standard approach nowadays especially in API design because it is fast and stateless which makes it easy to scale those APIs. The next type is O of2 authentication in combination with GVT tokens. So O of 2 is a protocol which is the second version of OAF. It lets users login through a trusted provider like Google or GitHub. So user sends a request to access your resources and if you allow them to authenticate with Google, basically Google sends your app a GVT token which contains the information of this user. This is how that payload will look like. Usually they send you the user ID or the email, the username and more stuff and also the expiration date for this GVT tokens. This is a signed object which then you pass from your app to the API and then your API will authenticate based on this information. Give are also stateless similar to bearer tokens which means that you don't need to store sessions between their requests and each request can be executed separately. Next we also have access and refresh types of tokens. So modern systems use shortlived access tokens which expire faster and also long lift refresh tokens which usually expire later than the access tokens. Access tokens are used for API calls. So whenever you want to get some data from the API, you send this access token to access the data and refresh tokens on the other hand are used to renew the access tokens. So whenever the access token expires, this is where you will use the refresh token to get a new one, a new access token behind the scenes. So this way users won't be logged out. They will stay logged in and also your system will stay secure because you are frequently renewing this access token. And one note here is that you should typically keep the refresh tokens in the server side for security reasons. And lastly we have SSO which stands for single sign on and identity protocols that are used with it. Single sign on lets users to have one login. So login once and access multiple services. For example, when you log into Google, you can access both Gmail, Drive, and also Calendar and all of their other services. And behind the scenes, this SSO uses either SL protocol or O of 2 protocol. Oaf2 is used more often nowadays for the modern applications to login with Google or with GitHub or any other service provider. It is a modern and JSON based. And on the other hand, SL protocol uses XML based approach. But still, SL is very popular in the legacy systems and in companies that use things like Salesforce or internal dashboards. So these are identity protocols which means that they will define how apps securely exchange the user login information between each other. But authentication is just the first step before users can access your service. So this tells you who the user is and if they are allowed to access your service. That is when they send a login request and you confirm or deny their identity. But after that you also have the authorization step which tells you what resources exactly this user can access to. Basically it tells you what they can do what the user can do in your system and that is what we will cover next in the next video. Before getting into authorization, there is a difference between how juniors would implement such authentication models and how seniors would implement it. Senior developers know that authentication is about securing tokens, refresh flows, and preventing attacks. And they also build it in a secure way while considering the tradeoffs. If you only know the theory, then companies will see it right through you. And if you want to implement those at a lower level with my guidance and one-on-one support, then that's why we have the mentorship program. If you're a developer with 1 to 5 years of commercial experience based in US, Canada, Europe, Australia, or New Zealand, and you're stuck in junior to mid-level roles, but you want to move into senior positions, master API design and other concepts and start earning senior level salaries, then you can apply to work with me oneonone. This is exactly what I help developers do inside of my mentorship program. We take these concepts and apply them hands-on in real world projects. The same way senior engineers will work in top companies. So the first link in description is where you can apply. But only apply if you're serious about advancing to senior roles. Otherwise, our calendar is fully booked all the time. And if your application doesn't seem like a good fit, then unfortunately we'll have to cancel it. Authorization is the step that happens after authentication. Once someone is logging in into our system. So once the login request is approved which means that the system now knows who the user is. The next step is deciding what they can do which is the step of authorization. It needs to check what resources or actions that user has permissions to access and also what are the denied actions for this user. This is how we control security and privacy in the systems. And in this video you'll learn how applications and systems manage permissions using the three main authorization models. The first one is role based access control. Next we have attribute based access control. Also access control list which is another way of managing authorization. Plus you'll learn how technologies like of2 and gvts help us to enforce those rules in practice. So authentication happens first which tells us who the user is and if they are allowed to access our system. But on the next step we have authorization which determines what you can actually do as a user in this system. If we take a look at GitHub as an example and accessing repositories on GitHub there you have different permissions for different users. For example, user A can have write access only which means they can only push code to this repo. But on the other hand, we can have user B and here you can grant only read access which means they can only read this repository but they cannot push code to it or they cannot create pull requests and so on. And on the other side we can have also admin users which have full control. So they can manage all the settings for the repository. They can even decide to delete this repository and so on. So you can see that different users can have different access controls on systems. To manage these access controls, we have common authorization models. So the one that we just looked at is the role based authentication model which assigns roles to users something like admin, editor or readonly access, write access. And this is the most common approach among these authorization models. But we also have attribute-based access control which is based on the user or resource attributes. So this is more flexible and more complex compared to the role-based authentication. And the other common approach is to have access control lists ACL and each resource here has its own permissions list. So you can assign permission lists to a resource and this is what will determine what resources you can access. For example, this is a common way of managing Google Docs and we will look at this in more detail now. And each of these models has its tradeoffs, pros and cons. So this depends on the specific system requirements. But real systems often combine also multiple models together to have more complex and more secure setup. So first up we have role- based access control or RBAC as an an acronym. Here users are assigned to roles and each role has a defined set of permissions. For example, as you saw with the GitHub, you can have admins and admins usually have full access to all resources. So they can create, they can read or update resources. They can even delete resources and also manage other users in the roles. And next you have editor which is usually a bit less than admin. So they can edit content like creating or reading content or updating resources but they cannot delete resources and they cannot also manage other users. And next you can have viewer users which can only read data. So they can read the resources and content but they cannot update anything or they cannot create anything in your system. This is the most common way in authorization models and this is used in apps that you use daily like you saw with GitHub or stride dashboards or CMS tools, team management tools and so on. The next model is attribute-based access control or ABAC in short. This access control goes beyond the roles. So it uses the user attributes or resource attributes and environment conditions to define the access. Some example policy you can see here. Let's say you want to only allow access if some conditions are met. In this case, whenever the user department is set to HR and you can combine this with multiple conditions like whenever the resource attribute equals to internal and so on and only in this case you allow them access and you either allow them read access or write access. So this can also be combined with the role based authorization but in this case you are checking the user model or resource model in your database and based on the attributes you either allow or deny the access. So here as you can see we are checking user attributes like the department the age or whatever you want to check here. Next, you can also combine it with resource attributes like confidentiality or the owner of the resource or classification. And this can also be combined with environment like time of the day, location, device type, and so on. Since you're combining these attributes to either grant or restrict access, this is more flexible than the role-based authorization, but it requires good policy management and generally it's more complex and you can encounter conflicts here with the attribute-based access control. The third common type is the access control lists. Instead of providing role based access or attribute-based access, you can have access control list for the specific resource. Let's say you have a resource like a document or a JSON file and here you can have a permission list on which users can access this document like user Alice has only read access or user Bob has both read and write access and another user has no access to this document. So as you can see we're managing two things here. First of all which users are allowed to access this document and second what are their permissions. So each of the users has different permissions on this document. ACL's are highly specific and also user centric which means it's hard to scale them well in systems with millions of users or objects unless you manage them carefully. But for example, Google Drive is one example of this where you have documents like a Google doc and then you share this Google doc with your colleagues, right? So you share someone with read access only and then you share this doc with someone else but now they can also edit and add comments to this document. So this is a example of ACL access control list which is used in Google drive and Google documents. This gives you more control over resources and documents, but it's also harder to scale with millions of users. But it's possible as you can see because Google Drive is using this for their documents, Excel sheets and so on. So these were the access control models. But how do systems enforce those authorizations? These are where O of 2 and GVt or access tokens come into play. So first we have OF2 which is delegated authorization which is a protocol used when service wants to access another services resources on a behalf of a user. For example, if you want to let a third party app read your GitHub repositories. Let's say you're deploying your app to Versel. So you need to give Versel control over your repository on GitHub. Instead of giving your username and password to the third party application which won't be secure at all because you don't know what they can do with your username and password. This way you are giving them full control. Instead, GitHub gives them the token that represents the permissions which you approved to use. So you as a user send a request with the third party app to request access to your repositories and then GitHub gives you the access token which you should create. So you should also provide what resources, what repositories this third party app can access and also what they can do. Can they create, read, update or can they delete or whatever the permissions you set and then GitHub sends them the token which contains the permissions which this third party app is allowed to use and OF2 defines the flow for securely issuing and validating those tokens. So you give them the access token and not your password which represents the permissions that you approve personally. So it can be reading specific repos or also creating pushing to those repositories but not deleting those repositories. And next we have also token based authorization using GVt or bearer tokens and permission logic. Once a user is authenticated, most systems use a token typically a GV token or this can be also bear token that carries this information like user ID, the roles like admin or editor and also scopes which is what scopes they are allowed to access and whenever this token is expiring and who is the issuer of this token. So whenever a user makes a request, it always carries this token information and reaches to the backend server. This is where the server will check your token and validity and it will apply the appropriate permission logic. So to not confuse this with authorization models, there is a key distinction. The token usually carries the identity and claims of your user as you see it here. But authorization models like role based or attribute-based this is what defines what is allowed to access as a user. So tokens are just mechanisms while these are authorization models. So in summary authorization isn't just letting users in like authentication but it also controls what they can access once they are in. We learned what authorization is, what are the three most common authorization models which are role based, attribute-based and access control lists. And also you saw a couple of real world examples like how GitHub manages your authorization tokens. And this should give you an idea on when to use each model based on the system that you're building. And you also saw some implementation patterns with O of 2 or GVT tokens. Each of these models has their own trade-offs, their own pros and cons, and real systems often combine multiple models to stay flexible and secure. APIs are like doors into your system. If you leave them unprotected, then attackers and anyone can walk right in and do whatever they want with your user data and overall the system. That's why in today's video, we'll look at seven proven techniques which will help you to protect your APIs from unwanted attacks. The first one we have in the list is rate limiting which controls how many requests a client can make in a given time. For example, you can set a limit for user A to make let's say 100 requests per some period of time to your API. And if they cross that limit and let's say make 101 requests, then you block the next request and allow some time to pass before they can send their next request. If you don't set this to your API, then attackers can overwhelm your system. They can send like thousands of requests per minute and then overwhelm your API which will take your system down or it can also brute force your data. And these rate limits can be set per endpoint. For instance, let's say you have some /comments endpoint and here they can send a request to either create a comment or fetch comments. You can set that limit for endpoint level. So these comments endpoint will be set to some strict number of requests per minute. You can also set it per user or IP address. Let's say in a we have the IP address of first user and then B for the second, C for this one and your attacker has some IP address which corresponds to D. If you get the 101 request from the D IP address, then you will know that this user overused the API. So you will block it at the user IP level. And there is also overall rate limiting to protect from DDOS attacks. Since you can set the rate limit to work per user or per IP address, that means that this attacker alone cannot send that many requests. You will block it with your rate limiting in the API. But what they can do is they can spin up some bots and each bot will have their own limit, right? Let's say you've set it to 100 per IP address. So each of these boats has 100 and overall they have more than you would allow or your system could handle. That's why you have also overall rate limitings which can be some bigger number. So whenever all the traffic coming into your server reaches or passes this number then you will temporarily block all requests until you find out the root cause. And of course these numbers are just examples. So in reality it's much more than thousand but that's just an example. The second one on the list is course which stands for cross origin resource sharing. This controls which domain can call your API from a browser and without proper course malicious websites could trick users browsers into making requests on their behalf. For instance, if your API is only meant to serve your front-end app which is at app.youdomain.com yourdommain.com then only requests from this source should be allowed. If anyone else sends you a request like up another domain.com then you should block this request and not allow them to use your API for authenticating or using any of its data. The third one is also a common one which is SQL and NoSQL injections. Injection attacks can happen when the user input is directly included in the database query. For instance, attacker can modify it and send some queries to read or delete your data. Here, for example, this part bypasses the checks entirely and then attacker can use this query to start reading data from your database or modify anything or they can also delete all the data, all the user data and any other tables that you have in this database. So to fix this, we always use parameterized queries or OM safeguards. The next technique to use is firewalls. Uh firewall acts as a gatekeeper filtering the malicious traffic from the other normal traffic. So typically you have it between your API and the incoming traffic. For example, if you use the AWS's web application firewall, these can block requests with unknown attack patterns such as suspicious SQL keywords or strange HTTP methods, which means it will block any suspicious requests from attackers, but it will allow others to bypass the request and reach to your API. Some APIs are also private and should only be accessed from specific networks. That's why we have also VPNs which stand for virtual private networks. The APIs that are within the VPN network can only be accessed by someone who is also within that same network. Which means that some APIs are public facing meaning these APIs will allow any requests from the internet from your users. But this for example can be within the VPN network. Which means if a user from web tries to reach your API then this request will be blocked because the user is not within the same network. But on the other hand if you have another user here which is within the VPN network they can make a request to these APIs and in this case they will bypass the checks and their request will reach to your APIs. This is useful where you have internal tools. Let's say you have internal admin dashboard and the API for this admin panel will only be reachable by employees connected to the company VPN. Next, we have CSRF, which stands for cross-sight request forgery. This tricks a logged in user's browser into making unwanted requests to the API. Let's say you as a user are logged in into your bank system and your bank system uses cookies for authentication. If the bank system is not secure and they only use session cookies, another malicious site might use your cookie and submit a hidden transferring money request through your cookie. So to prevent such attacks, companies also use CSRF tokens in combination with session cookie. So the banking system will check if the session cookie is present but it will also check if the CSRF token matches with the one that they have and if it doesn't then it will block this request from the other unknown source while it will allow request from your behalf. And the last one we have is XSS or it's also called cross-sight scripting. This lets attackers to inject scripts into web pages served to other users. For example, if you have a comment section and this comment gets submitted to your API. Next, your API will also store it in a database. You can get normal requests like nice picture or something like that and this will get to your API. Your API will store it in the database. So everything is fine there. But what if an attacker places a script in this comment section and within this script they can try to do many different things. For example, they can try to fetch the cookie for another user or they can try to inject something into your database. And if you allow this, then it will reach to your server and the information will be written into the database. Later when the other users load these comments section on their screen, they will get also the injected comment directly into their web page and the browser will execute this malicious JavaScript code into the other users browser. Before ending the video, again reminding you about the mentorship program. If you're a developer with one to five years of commercial experience based in US, Canada, Europe, Australia, or New Zealand, and you're stuck in junior to mid-level roles, but you want to move into senior positions, master API design and other concepts and start earning senior level salaries, then you can apply to work with me oneon-one. This is exactly what I help developers do inside of my mentorship program. We take these concepts and apply them hands-on in real world projects. The same way senior engineers will work in top companies. So the first link in description is where you can apply. But only apply if you're serious about advancing to senior roles. Otherwise, our calendar is fully booked all the time. And if your application doesn't seem like a good fit, then unfortunately we'll have to cancel



0:00
Hey, my name is Milan. I'm a software
0:01
architect and Microsoft MVP. And in this
0:04
video, I'm going to share eight tips
0:05
that are going to help you design a
0:07
better REST API. I'm also going to show
0:10
you a complete implementation example
0:12
that apply these tips in practice. So,
0:15
let's dive in as we have a lot of topics
0:17
to cover. What I'm going to share with
0:19
you in this video can be considered some
0:21
best practices when it comes to
0:23
designing a REST API. So, I have to
0:25
preface this by saying that you should
0:27
approach all best practices with a dose
0:30
of pragmatism. You should definitely
0:31
consider every tip that I share as part
0:34
of this video because this is also what
0:36
I'm using in my REST APIs that I'm
0:38
running in production, but also
0:40
scrutinize them and figure out if they
0:42
make sense for you. This is what being
0:44
pragmatic is all about where you get to
0:47
decide what is the correct decision for
0:49
your context. Now, with that out of the
0:51
way, let's take a look at the first tip.
0:53
So tip number one is going to be super
0:54
simple. It's just be consistent.
0:57
Whatever design approach that you decide
0:59
to implement in your REST APIs, be
1:01
consistent across the board. If you're
1:04
using plural nouns, use plural nouns
1:05
everywhere. If you are using 400 bad
1:07
requests for validation responses, then
1:09
use that status code everywhere. Being
1:11
consistent helps you in two ways. First
1:13
of all, it allows you to iterate faster
1:16
as you already know what design approach
1:18
you need to take when introducing a new
1:20
API. Secondly, it also makes it easier
1:22
for your API consumers because once they
1:24
understand how your API is designed, for
1:26
example, how to construct URIs and what
1:28
status codes and responses they should
1:30
expect, it makes it easier for them to
1:32
integrate with your API. So, tip number
1:34
one is be consistent. Tip number two is
1:37
that you should support at least the
1:38
standard set of HTTP methods. These are
1:41
going to be post, put, get, and delete.
1:44
Anything other than this is up for
1:46
debate and you can decide if you want to
1:48
use it for your API, but the four
1:50
standard HTTP methods you should
1:52
definitely be using in your REST API
1:54
implementations. These also happen to be
1:56
the most common HTTP methods, even
1:58
though there are a bunch of them that
2:00
are somewhat exotic and rarely used. I'm
2:02
not really going to mention them, but
2:03
get is used when you want to retrieve a
2:06
resource from an API. Post is used when
2:08
you want to create a new resource. put
2:10
is used to either update a new resource,
2:13
but it can also be used for an upsert
2:15
where you store the resource if it
2:17
doesn't exist. This is an implementation
2:19
detail and you get to decide what put
2:21
does in your API. Delete is used for
2:24
removing a resource and then patch is
2:26
used for a partial update. In practice,
2:28
I've rarely seen patch used in
2:30
production. It's mostly either a
2:31
combination of get post, put, and
2:32
delete. And if you want to have some
2:34
specialized endpoints for partial
2:36
updates, it's not uncommon to use the
2:38
boot HTTP method combined with a
2:40
dedicated URI for that partial update.
2:42
Tip number three continues along the
2:44
same idea and it's returning the correct
2:47
HTTP status codes for the API response
2:50
that you want to represent. Here are the
2:52
five most important groups of HTTP
2:54
status codes. We haveformational status
2:56
codes in the 100 range, successful
2:58
status code in the 200 range,
2:59
redirection status codes in the 300
3:01
range, client errors in the 400 range,
3:04
and then server errors in the 500 range.
3:06
Out of these, the most important ones
3:08
are going to be 200 okay 21 created, 202
3:11
accepted, 204 no content, and then
3:14
you'll typically see something like 400
3:15
bad request, 401, 403, 404, 409 is not
3:20
that often used, although I like to use
3:22
it for example when you run into a
3:24
conflict on something like a distributed
3:26
lock or you fail some unique constraint.
3:28
A simple example is email uniqueness.
3:30
Lots of people like using 422
3:32
unprocessable content or unprocessible
3:34
entity to represent validation failures.
3:37
I'm not a huge fan of this myself. I
3:39
typically just return 400 bad requests
3:41
for any validation error and I also
3:43
return 400 bad requests for business
3:45
failures. I think this approach makes my
3:47
API somewhat simpler, but you do you and
3:49
figure out what makes sense. 429 too
3:51
many requests is used when you're
3:53
running to a rate limit. Usually, you
3:55
don't want to be returning 500 errors
3:57
yourself. This is typically reserved for
3:58
the server if you really run into a
4:00
problem, but nonetheless, you should at
4:02
least understand what are the most
4:04
popular HTTP status codes and then when
4:06
it's appropriate to use them. Moving on
4:08
to tip number four. It's about naming
4:11
your resources. The best practice here
4:13
that I like to follow is using plural
4:15
nouns for your resource name. So, it's
4:17
essentially preferring to use something
4:19
like habits to represent the collection
4:21
resource versus just the singular noun
4:23
of habit. Now, when you think about it,
4:25
it does make sense because the plural
4:27
noun does represent multiple resources,
4:30
and it's what you would expect when
4:31
calling the habits endpoint. You expect
4:33
this to return a collection of resources
4:36
instead of just a single resource. This
4:38
is definitely a simple best practice,
4:39
but I think it's one you should adopt in
4:41
your REST APIs. The next tip is about
4:44
avoiding deeply nested URLs. To give you
4:47
an example, here's an ownership
4:48
hierarchy where we have the users
4:50
resource and then a specific user
4:52
represented with their identifier and
4:54
then a habits resource which means the
4:57
habits that belong to this specific user
4:59
and then again we're scoping this down
5:01
to a single habit and finally we have
5:03
the entries for that specific habit for
5:05
that specific user. So you can see how
5:07
this URI already becomes pretty
5:08
unwieldy. you have two levels of nesting
5:10
and it's not impossible that we would
5:12
encounter more levels of nesting and
5:14
this is something that I like to avoid
5:15
in my APIs. So what you could do instead
5:18
is just expose a tople resource for the
5:20
entries and then allow this resource to
5:22
be filterable by the habit ID. The habit
5:24
already belongs to this user. So if you
5:27
know the habit ID for this user then you
5:29
already know that it's implicitly
5:30
applied when you filter the entries by
5:32
the habit ID. I don't like imposing any
5:34
strict rules when it comes to how much
5:36
nesting is too much. But typically,
5:39
whenever you're running to two levels of
5:41
nesting, I consider it a red flag that I
5:43
should probably reconsider my API
5:45
design. Tip number six is going to be a
5:47
bit controversial as I suspect that not
5:49
many people will agree, but it's
5:51
wrapping your collection responses or
5:53
arrays in an envelope. So, what's the
5:55
idea here? Let's say we have our
5:57
collection resource like returning a
5:59
list of entries and instead of returning
6:00
this as a collection or an array in
6:03
JSON, you return a JSON object
6:06
containing a data property that contains
6:08
the actual collection. So what's the
6:10
idea behind this? Well, it lets you do a
6:12
couple of things. The main benefit here
6:14
is optionality. If you return an object
6:17
for your collection resources, you now
6:18
have the ability to easily evolve this
6:20
API in the future. If you want to add
6:22
pagionation metadata to the response,
6:24
you can do so without breaking your API
6:27
consumers. If you want to include
6:28
hyperdia links, you can also do that
6:30
without breaking your consumers. So this
6:32
is definitely something you should
6:33
consider when designing your APIs. Think
6:36
about how you can design them with
6:37
optionality in mind so that you can
6:39
easily evolve this API in the future
6:42
when your requirements will change and
6:44
they most certainly will over time. Tip
6:46
number seven is you should use
6:48
pageionation for your collection
6:50
resources. You get to decide which
6:52
pagionation approach you want to use.
6:53
The most popular ones are offset
6:55
pagenation where you specify some offset
6:58
representing how many records you want
7:00
to skip and then a limit parameter
7:02
representing how many records you want
7:03
to return. You will often see these
7:05
parameters called skip and take or page
7:08
and page size or page number and page
7:09
size. It doesn't matter. It's all the
7:11
same idea. You skip a certain number of
7:13
records and then you return the records
7:15
after the ones you skip. This also
7:17
relies on using the skip tech support in
7:19
most relational databases to implement
7:21
offset based pagenation. Cursorbased
7:23
pagenation is very interesting because
7:25
instead of specifying a fixed offset for
7:28
how many records you want to skip, you
7:30
define a cursor which represents a sort
7:33
of a logical pointer which determines
7:35
the exact record from which you want to
7:37
take the next page of data. Cursor
7:39
pagenation is also called key set
7:41
pagionation and it has the added benefit
7:43
that it's significantly faster than
7:45
offset pagionation on large data sets
7:47
especially when accessing the later
7:49
pages in the data set itself. And we're
7:51
still on the topic of pagenation but I
7:53
want to tie it into our previous tip
7:55
which is wrapping our collection
7:57
responses in an envelope and you can see
7:59
an example of that here. We are
8:00
returning our collection in the data
8:02
property and then we also have an
8:04
additional field in the JSON response
8:06
representing our hypermedia links. And
8:08
this is especially useful for
8:09
pagionation because after you fetch the
8:12
current page, you already know how to
8:14
construct a URI to fetch for example the
8:16
previous page or the next page or the
8:18
first page or the last page depending on
8:20
the pageionation approach that you use.
8:22
In this example, I have two hypermedia
8:23
links here. one pointing to the first
8:26
page represented by this link here and
8:28
then the second hypermedia link pointing
8:30
to the next page of data after the one
8:32
that we currently have. And this allows
8:34
your API clients to easily navigate
8:36
through your API where they can fetch
8:37
the first page. Then they have the link
8:39
to send a request to fetch the next page
8:41
and so on until they reach the end of
8:43
the data set. Tip number eight is using
8:45
a structured error format. It doesn't
8:47
matter which one you decide to use. I
8:49
prefer using problem details because
8:51
it's a standard and it's wellnown. You
8:53
can see an example of that here. It's
8:55
defined with a custom content type
8:57
called application/p problem JSON. And
8:59
then you can see the example of what a
9:02
response like that would look like in
9:03
JSON. There's the type property which
9:06
would point to some documentation
9:07
explaining which type of error this is.
9:09
A title that's something short and
9:11
descriptive explaining what the error
9:12
is. The respective status code which
9:14
would be the same status code that you
9:16
use for the HTTP response. Then the
9:18
instance this can point to the resource
9:20
that you tried to access. The trace ID
9:22
and the request ID are optional values
9:24
and you can add them to improve
9:25
debugging. If you don't want to use the
9:27
problem detail standard, then that's
9:29
perfectly acceptable. Just pick some
9:31
sort of structured error format that you
9:33
want to use and stick to that for all of
9:35
your failure responses that you return
9:37
from your REST API. And this concludes
9:39
my eight tips for designing a better
9:42
REST API. Now, I want to show you an
9:44
implementation example that applies all
9:46
of this in practice. What you're looking
9:48
at is the dev habit application which is
9:50
a restful API for managing personal
9:53
habits and their respective entries.
9:55
Right here we have the entries
9:56
controller. I'm using controllers in
9:58
this example and you can already start
10:00
to see some of the best practices. For
10:02
example, I'm using plural nouns for the
10:04
route. I'm using API versioning. I also
10:06
have rate limiting enabled and
10:07
authorization using roles. Then I'm
10:10
improving my API documentation by
10:12
defining what response types this API
10:15
produces in terms of status codes and
10:16
content types. And then each API
10:18
endpoint inside of this controller or
10:20
controller method is going to follow
10:22
these similar best practices. We have
10:24
good documentation in place. We're using
10:27
the correct HTTP methods. In this case,
10:29
HTTP get. We are documenting which
10:31
status codes and response types you can
10:33
expect from this API endpoint. And in
10:36
this example, it's a collection
10:37
response. So you can see that at the end
10:40
we are returning a pagionation result
10:42
and we also have an option to include
10:44
the hypermedia links. Another example
10:46
here implements cursorbased pagenation.
10:48
You can see this is defined as a
10:50
separate resource and again we're just
10:52
implementing pageionation and returning
10:53
an envelope response to the API client.
10:56
Then we have an endpoint for fetching a
10:58
single entry. Notice how we are
10:59
returning multiple different status
11:01
codes. In case of some validation
11:02
problem, we return a problem details and
11:04
some descriptive message of Bob and
11:06
wrong. If we can't find the resource, we
11:08
return 404 not found. And if everything
11:10
checks out, we return 200. Okay. In the
11:13
post endpoint, we follow the respective
11:15
semantics. And in case of the post
11:17
method, this is returning 2011 created
11:19
as the correct status code. We also
11:21
include the URI pointing to the newly
11:24
created resource. This will be present
11:25
in the location header on the response.
11:27
Then you can see similar best practices
11:29
followed in the remaining endpoints such
11:31
as HTTP put. Here's an example of some
11:33
partial updates. And then here's an
11:35
example of HTTP delete. And in the end,
11:37
I want to show you how I construct the
11:39
hypermedia links. You can see it here.
11:41
I'm using the link service, which is an
11:42
abstraction that's available in ASP.NET
11:44
Core. And you can use it to construct
11:46
fully qualified URIs pointing to your
11:48
API endpoints. Now, this entire
11:50
application called dev habit is what I
11:52
implement from scratch inside of
11:53
pragmatic REST APIs. And lastly, I want
11:56
to leave you with a useful resource. At
11:58
least I found it useful when I was
12:00
researching REST APIs and I want to
12:02
share it with you. And that is the
12:04
Zalando restful API guidelines. It's a
12:07
set of guidelines and best practices
12:09
similar to what I shared in this video
12:11
from a company called Zando. And I'm
12:13
going to leave it in the description
12:15
below this video. It contains a bunch of
12:17
useful information when it comes to
12:19
designing REST APIs. And I think you're
12:21
going to find it very useful and
12:23
relevant to the topic that we are
12:24
talking about in this video. Also, if
12:26
you have suggestions or know of any
12:28
other popular REST API design
12:30
guidelines, feel free to drop them in
12:32
the comments below this video. We were
12:34
talking about designing REST APIs this
12:36
entire video, but do you actually know
12:38
what a REST API is? Take a look at this
12:40
video next to learn about the Richardson
12:43
maturity model and see if your API
12:46
qualifies as a restful API. Thanks a lot
12:49
for watching this video. Consider
12:51
smashing the like button for the YouTube
12:53
algorithm so that this gets distributed
12:55
to more .NET developers. And until next
12:57
time, stay awesome.


Introduction to FastAPI Best Practices
0:00
In this video, we'll look at
0:01
15 essential best practices for FastAPI.
0:04
These are tips every FastAPI developer
0:07
should know, covering everything from `async`
0:09
rules and database connections to logging,
0:12
background tasks, and even deployments.
0:14
By following these, you'll be able
0:16
to avoid common mistakes and build
0:18
applications that are both high-performing and
0:21
truly production-ready.
Avoid Async for Blocking Operations
0:22
Number One: Never use async def
0:24
for blocking operations.
0:26
In Python, blocking operations are tasks
0:28
that make your program wait until
0:30
they're finished before moving on. For
0:32
example, time.sleep(10) will pause your program
0:34
for 10 seconds.
0:35
Other common examples include reading from
0:38
or writing to files
0:39
making HTTP requests with libraries like
0:42
requests
0:42
or performing database queries using synchronous
0:45
clients such as MongoClient from pymongo.
0:48
Now, here's the crucial part: FastAPI
0:50
runs endpoint functions defined with async
0:53
def in the main thread.
0:55
If you put these blocking operations
0:56
inside an async def function, your
0:59
application will become unresponsive. It won't
1:02
be able to process other requests
1:03
until that blocking operation finishes.
1:06
The solution is simple: define these
1:08
functions using a regular def keyword
1:10
instead of async def.
1:12
FastAPI is designed to recognize that
1:14
you're performing blocking operations within these
1:16
def endpoints and will intelligently run
1:19
them in separate threads. This way,
1:21
your FastAPI application remains responsive.
Use Async Friendly Code
1:24
Number two, use async friendly code.
1:26
To get the most performance out
1:28
of your FastAPI application, use non-blocking
1:30
libraries as much as possible so
1:33
you can declare your endpoints as
1:34
async endpoints.
1:36
Use asyncio.sleep instead of time.sleep
1:39
httpx.AsyncClient instead of the requests module
1:42
and motor instead of pymongo. I
1:44
hope you get the idea.
Avoid Heavy Computation in Endpoints
1:46
Number three, don't do heavy computation
1:48
in FastAPI endpoints, or you'll block
1:50
your server.
1:51
FastAPI is built for I/O-bound workloads.
1:54
So, avoid processing images
1:55
or videos
1:56
or running heavy machine learning models
1:58
directly in your endpoints.
2:00
The reason is that your application
2:01
will be unresponsive for as long
2:03
as the computation runs.
2:04
What should you do instead?
2:06
If your ML model is lightweight
2:08
and inference is fast—say, under 100ms
2:11
with low traffic
2:12
or if you're just prototyping, you
2:14
can get away with using FastAPI
2:16
directly.
2:16
But for heavier ML models
2:18
use dedicated inference engines like Triton,
2:21
TensorFlow Serving, or TorchServe
2:23
and then use FastAPI to validate
2:25
inputs and route the request.
2:27
For truly long-running computations, use a
2:29
queue-plus-worker system.
2:31
When the client sends a request
2:32
to FastAPI
2:33
FastAPI enqueues a job to a
2:35
message broker like RabbitMQ.
2:37
A separate worker (e.g., Celery) pulls
2:40
the job from the queue, runs
2:41
the heavy computation (on CPU or
2:43
GPU nodes)
2:44
and stores the result in a
2:45
database.
2:46
By the way if your FastAPI
2:47
app is slow
2:48
and you'd like me to take
2:49
a look
2:49
just send me an email here.
FastAPI Dependencies Rules
2:51
Number four, the previous rules for
2:53
endpoints also applies to Fastapi dependencies.
2:55
Define your dependency with def if
2:58
you're doing any blocking operation inside.
3:01
Use async def if it's light,
3:02
CPU-bound, and entirely non-blocking.
3:05
And don't perform any heavy computation
3:07
inside your dependency.
3:09
Number Five, Don't make your users
Don't Make Users Wait (Background Tasks)
3:11
wait.
3:11
When you have operations that don't
3:13
need to block the client from
3:14
getting a response—things
3:15
like sending a confirmation email
3:17
or logging event
3:18
you should avoid making the client
3:20
wait by doing it in the
3:21
endpoint.
3:22
Instead, use FastAPI's built-in BackgroundTasks. They're
3:25
perfect for small, non-critical, "fire-and-forget" tasks.
3:28
Since these background tasks run within
3:30
the same event loop as your
3:32
main application
3:33
you'll want to follow the same
3:34
rules for deciding between async def
3:36
and def when defining them.
3:38
However, it's crucial to understand the
3:40
limitations: don't use BackgroundTasks for anything
3:43
requiring guaranteed delivery, retries, or tasks
3:46
that run for a long duration.
3:48
For those more robust needs, a
3:50
dedicated message queue and worker system
3:52
(like Celery) is still the superior
3:54
choice. Remember, if your FastAPI process
3:56
crashes before a background task completes,
3:58
that task will fail.
Don't Expose Swagger/ReDoc in Production
4:00
Number six, don't expose Fastapi Swagger
4:03
or ReDoc in production unless your
4:05
API is public-facing. FastAPI automatically generates
4:08
these docs, which is great during
4:10
development.
4:10
But in production, they can reveal
4:12
endpoints that might still be incomplete
4:14
or lack proper security.
4:15
To prevent this, make sure to
4:17
set docs_url, redoc_url, and openapi_url to
4:21
None in your production settings.
Create Custom Pydantic Base Model
4:22
Create a custom base model and
4:25
use it to define your Pydantic
4:26
models.
4:27
The benefit is that it's easier
4:29
to manage your global configuration in
4:30
one place.
4:31
For example, if you want to
4:32
return camelCase to the frontend but
4:34
prefer snake_case in Python, you can
4:36
set an alias generator to handle
4:38
that.
4:38
Another common use case is simplifying
4:40
JSON encoding when returning data. If
4:42
you try to return a datetime
4:44
or Decimal object, you'll get an
4:46
encoding error. So, define your encoders
4:48
globally.
4:48
You can format datetime objects as
4:50
strings,
4:51
convert Decimal to float,
4:52
and if you're working with MongoDB,
4:54
convert ObjectId instances to strings.
4:56
Number eight, don't manually construct response
4:59
models in your FastAPI endpoints.
Don't Manually Construct Response Models
5:00
If you've set a response_model, there's
5:03
no performance benefit
5:03
to creating that model yourself just
5:05
to return it.
5:06
FastAPI will do it anyway.
5:08
It first converts your return value
5:10
to a dictionary or a list
5:11
validates it against the response_model
5:14
and then encodes it to JSON.
5:15
Just return a plain dictionary, and
5:17
let FastAPI take care of the
5:19
rest.
5:19
Number nine, validate with Pydantic and
5:22
Not Your Code.
Validate with Pydantic, Not Your Code
5:23
Push as much validation and structure
5:26
definition as possible into your Pydantic
5:28
models. That's what they're built for.
5:30
Don't scatter validation across your route
5:31
functions with if statements and manual
5:33
checks. It might seem easier at
5:35
first, but it quickly turns into
5:37
a mess. You lose consistent error
5:39
responses, and clients get cryptic 400s
5:42
with no clue why their request
5:43
failed. You end up repeating the
5:45
same logic in multiple places. And
5:47
the worst part—your OpenAPI docs won't
5:50
reflect any of that hidden validation.
5:52
If Pydantic doesn't support the validation
5:54
you need, don't write it inside
5:56
the endpoint function—add custom validation logic
5:58
directly inside the model instead.
6:00
Number ten, Use dependencies for DB-based
Use Dependencies for DB-Based Validation
6:03
validation.
6:04
If your validation logic requires a
6:06
database query—like checking whether a resource
6:09
belongs to the current user before
6:11
allowing changes—don't put that logic directly
6:13
in your endpoint.
6:14
Instead, create a dependency and use
6:16
it.
6:16
The first benefit is that you
6:18
can easily reuse the dependency across
6:20
multiple endpoints with just a single
6:21
line of code.
6:22
And there's no performance hit, since
6:24
FastAPI caches dependencies per request
6:26
your dependency will be evaluated only
6:28
once in a request. Overall, it's
6:30
efficient and helps keep your endpoints
6:32
clean.
Avoid New DB Connections in Every Endpoint
6:33
Number eleven, avoid creating a new
6:35
database connection in every endpoint.
6:37
Instead, you should use a connection
6:39
pool and access these connections through
6:41
dependency injection. There are two common
6:43
ways to do this:
6:44
The first one is storing the
6:45
db connections pool in the app
6:47
state. You initialize the connection pool
6:49
within the lifespan function and then
6:51
store it in app.state.
6:52
After that, you create a dependency
6:54
that retrieves a connection from this
6:56
pool. You'll typically use an async
6:58
with block to ensure the connection
7:00
is properly released. This method makes
7:02
it straightforward to clean up resources
7:04
during shutdown, which is particularly helpful
7:07
if you're working with multiple databases.
7:09
Second, the legacy global-style pool. This
7:12
approach involves defining the connection pool
7:14
as a global variable and populating
7:17
it during the app's lifespan event.
7:19
While the first method (storing in
7:21
app state) is the recommended approach,
7:22
you'll still frequently encounter the global-style
7:25
pool in many existing codebases.
Use Lifespan Events for Resource Management
7:27
Number twelve, use the new lifespan
7:29
event for managing app level resources.
7:32
It's better to use FastAPI's newer
7:34
lifespan feature
7:35
instead of the older @app.on_event("startup") and
7:37
@app.on_event("shutdown") decorators.
7:40
Lifespan gives you a single, unified
7:42
context to handle setup—like initializing database
7:45
connections, cache clients, or starting background
7:48
tasks—and cleanup when the app shuts
7:50
down. It keeps your startup and
7:52
shutdown logic together, which makes things
7:54
easier to manage and reduces the
7:55
chance you'll forget to clean something
7:57
up.
7:57
And if startup fails, lifespan still
7:59
ensures cleanup happens—which isn't guaranteed with
8:01
the old way.
Don't Hardcode Secrets
8:02
Number thirteen, don't hardcode secrets in
8:04
your code
8:05
use env files and config classes
8:07
to access them.
8:07
Never hardcode values like passwords, API
8:10
keys, tokens , et cetera directly
8:12
in your source code.
8:13
Instead, use a .env file—and make
8:16
sure to
8:16
add .env to your .gitignore so
8:19
it doesn't end up in version
8:21
control.
8:21
It's a good idea to include
8:22
a .env.example file in your repo.
8:25
This acts as a template, showing
8:27
what environment variables are expected, without
8:29
exposing any real secrets. Keep it
8:31
up to date—it should closely mirror
8:33
the actual .env structure.
8:34
That said, try not to overload
8:36
your .env file with things that
8:38
control business logic or complex behavior.
8:40
Keep it focused on static configuration—stuff
8:43
like port numbers, DB URLs, credentials,
8:46
and simple toggles like DEBUG=true. Anything
8:49
more dynamic is usually better off
8:51
handled directly in code.
8:52
Also, avoid accessing os.environ all over
8:55
your codebase. It gets messy fast.
8:58
A better approach is to centralize
8:59
everything in a config class—something like
9:02
a Settings class. In my projects,
9:03
I use Pydantic's BaseSettings. It validates
9:06
all your environment variables upfront. If
9:08
something's missing or misconfigured, it fails
9:10
early right when the server starts,
9:12
and it makes switching between development
9:14
and production environments a lot smoother.
9:16
Another solid option is Dynaconf. It's
9:19
more flexible and supports things like
9:20
multiple environments and layered settings
9:23
Number fourteen, use structured logging and
9:26
avoid relying on simple print statements
9:28
for logging.
Use Structured Logging
9:29
The print() function simply doesn't offer
9:31
the control and flexibility we need.
9:33
For instance, there's no built-in way
9:35
to specify a log level or
9:36
automatically add context like request id,
9:39
timestamps. Debugging with print() can quickly
9:41
become a nightmare because you can't
9:43
easily filter out irrelevant messages—every print
9:46
statement has the same visibility.
9:47
Instead, I highly recommend using Python's
9:50
standard logging module, often paired with
9:52
powerful libraries like Loguru or Structlog.
9:54
These libraries allow you to assign
9:56
a criticality to your logs, whether
9:58
it's something for debugging, an informational
10:00
message, a warning, an error, or
10:03
a critical alert. This becomes incredibly
10:05
useful when you're running your application.
10:08
Depending on the environment, whether it's
10:10
development or production, you can set
10:11
the log level. For example, you
10:13
might set it to DEBUG or
10:15
INFO or ERROR, which gives you
10:17
fine-grained control over what messages you
10:19
see.
10:19
In development, you'll likely want to
10:21
see everything, so setting the log
10:22
level to DEBUG is common.
10:24
However, in production, you'll probably want
10:26
to set the log level higher,
10:28
perhaps to INFO or ERROR, based
10:30
on your specific operational needs.
10:32
Another incredibly useful practice is to
10:34
add contextual information to each log,
10:37
such as request ID. Unlike print()
10:39
statements, where you'd have to manually
10:41
add this to every single one
10:42
libraries like Structlog allow you to
10:45
create a middleware that stores context
10:47
variables. These context variables safely pass
10:50
request-specific data across
10:51
asynchronous calls.
10:52
Then, within your logger configuration, you
10:54
can configure it to include the
10:56
context variables, time, log level, and
10:58
the actual message.
10:59
The request ID is just a
11:01
simple example. You might also want
11:02
to log other contextual information like
11:05
the user ID.
11:06
Just make sure you're never logging
11:07
anything sensitive, like database credentials or
11:10
API keys.
11:11
Finally, if you're running multiple instances
11:13
of your application, it's a good
11:14
practice to centralize your logs. To
11:16
do this, you'll likely want to
11:18
push these JSON logs using Filebeat
11:20
to Elasticsearch for centralized storage and
11:23
analysis.
Best Practices for Deploying FastAPI
11:24
And for our last point, number
11:26
15, we're talking about the best
11:28
practices for deploying FastAPI.
11:29
For local development, Uvicorn on its
11:32
own is perfectly fine. However, when
11:33
you move to a production environment,
11:35
you should run Uvicorn with Gunicorn,
11:37
specifically using the UvicornWorker class.
11:40
Next, make sure to install uvloop
11:42
in your Python virtual environment. FastAPI
11:44
will automatically detect and use uvloop
11:47
instead of the default asyncio event
11:49
loop, which can provide a significant
11:51
performance boost.
11:52
Finally, you'll want to tune the
11:54
number of workers to match your
11:55
server's CPU. A common starting point
11:57
is to set workers equal to
11:59
(CPU cores * 2) + 1.
12:00
However, this isn't a hard and
12:02
fast rule; you should experiment and
12:04
benchmark to find the optimal number
12:06
that maximizes your server's performance.
12:08
If you're looking to scale your
12:10
application further, consider containerizing it using
12:13
Docker.
12:13
Don't forget to drop me an
12:15
email here, if you want me
12:16
to review your application




0:00
in this tip TI we'll go through the API
0:02
design starting from the basics and
0:04
advancing towards the best practices
0:06
that Define exceptional apis as a
0:09
developer you're likely familiar with
0:11
many of these Concepts but I'll provide
0:13
a detailed explanation to deepen your
0:15
understanding let's consider an API for
0:18
an e-commerce platform like Shopify
0:20
which if you're not familiar with is a
0:22
well-known e-commerce platform that
0:24
allows businesses to set up online
0:25
stores in API design we are concerned
0:28
with defining the inputs like product
0:30
details for a new product which is
0:32
provided by a seller and the outputs
0:35
like the information returned when
0:37
someone queries a product of an API so
0:40
the focus is mainly on defining how the
0:42
crowd operations are exposed to the user
0:45
interface CR stands for create read
0:47
update and delete which are basic
0:50
operations of any data driven
0:52
application for example to add a new
0:55
product we need to send a post request
0:57
to/ API SL products where the product
1:00
details are sent in the request body to
1:03
retrieve these products we need to send
1:04
the get request to/ API SL products for
1:08
updating we use put or patch requests
1:11
to/ products SL the ID of that product
1:15
and removing is similar to updating it's
1:17
again/ product/ ID of the product we
1:20
need to remove and similarly we might
1:23
also have another get request to/
1:25
product/ ID which fetches the single
1:28
product another part is to decide on the
1:30
communication protocol that will be used
1:33
like HTTP websockets or other protocols
1:36
and the data transport mechanism which
1:38
can be Json XML or protocol buffers this
1:42
is usually the case for restful apis but
1:45
we also have graphql and grpc paradigms
1:48
so apis come in different paradigms each
1:51
with its own set of protocols and
1:53
standards the most common one is rest
1:56
which stands for representational State
1:58
transfer it it is stateless which means
2:01
that each request from a client to a
2:03
server must contain all the information
2:05
needed to understand and complete the
2:07
request it uses standard HTTP methods
2:10
get post put and delete and it's easily
2:14
consumable by different clients browsers
2:17
or mobile apps the downside of restful
2:20
apis is that they can lead to over
2:22
fetching or under fetching of data
2:24
because more endpoints may be required
2:26
to access specific data and usually rest
2:30
apis use Json for data
2:32
exchange on the other hand graphql apis
2:35
allow clients to request exactly what
2:37
they need avoiding over fetching and
2:39
under fetching data they have strongly
2:42
typed queries but complex queries can
2:45
impact server performance and all the
2:48
requests are sent as post requests and
2:51
graph coal API typically responds with
2:54
HTTP to un status code even in case of
2:57
errors with error details in the
2:59
response body grpc stands for Google
3:02
remote procedure call which is built on
3:04
HTTP 2 which provides Advanced features
3:07
like multiplexing and server push it
3:10
uses protocol buffers which is a way of
3:13
serializing structured data and because
3:16
of that it's sufficient in terms of
3:17
bandwidth and resources especially
3:19
suitable for
3:21
microservices the downside is that it's
3:23
less human readable compared to Json and
3:26
it requires http2 support to operate
3:30
in an e-commerce setting you might have
3:32
relationships like user to orders or
3:34
orders to products and you need to
3:37
design endpoints to reflect these
3:39
relationships for example to fetch the
3:41
orders for a specific user you need to
3:43
query to get/ users SL the user id/
3:47
orders common queries also include limit
3:50
and offset for pagination or start and
3:53
end date for filtering products within a
3:55
certain date range this allows users or
3:58
the client to retrieve specific sets of
4:01
data without overwhelming the system a
4:04
well-designed get request should be IDM
4:06
ponent meaning calling it multiple times
4:09
doesn't change the result and it should
4:11
always return the same result and get
4:14
requests should never mutate data they
4:16
are meant only for retrieval if you need
4:18
to update or create a data you need to
4:21
do a put or post request when modifying
4:24
end points it's important to maintain
4:26
backward compatibility this means that
4:28
we need to ensure that changes don't
4:30
break existing clients a common practice
4:33
is to introduce new versions like
4:35
version two products so that the version
4:38
one API can still serve the old clients
4:41
and version 2 API should serve the
4:43
current clients this is in case of
4:45
restful apis in the case of graphql apis
4:49
adding new Fields like V2 Fields without
4:51
removing old one helps in evolving the
4:54
API without breaking existing clients
4:57
another best practice is to set rate
5:00
limitations this can prevent the API
5:02
from Theos attacks it is used to control
5:05
the number of requests a user can make
5:07
in certain time frame and it prevents a
5:10
single user from sending too many
5:12
requests to your single API a common
5:16
practice is to also set course settings
5:18
which stands for cross origin resource
5:21
sharing with course settings you can
5:23
control which domains can access to your
5:25
API preventing unwanted cross-site
5:28
interactions API design is just one part
5:31
of system design interview if you'd like
5:33
to learn more about the other Concepts I
5:35
recommend you check out this system
5:37
Design Concepts next





0:00
REST API. Boom. This is the classic one.
0:04
Rest is like the old reliable pizza
0:06
delivery guy of the internet. You send a
0:08
request like get me all users and boom,
0:11
it delivers clean JSON data to your app.
0:14
It uses simple HTTP methods. Get for
0:18
reading, post for sending, put for
0:21
updating, delete for removing. Almost
0:24
every website you use, Instagram,
0:26
YouTube, Twitter, runs on REST. It's
0:29
simple, predictable, and easy to debug.
0:32
But here's the catch. Sometimes REST
0:35
gives you too much data, even stuff you
0:37
don't need. Let me fix that with the
0:39
next one. GraphQL. GraphQL said, "Rest,
0:43
you're cool, but I want control." With
0:46
GraphQL, you ask exactly for what you
0:49
want. Nothing more, nothing less. Like,
0:52
hey API, just give me the username and
0:55
profile picture. And boom, it gives
0:58
exactly that. It's super efficient,
1:00
especially for big apps with lots of
1:02
data used by Facebook, GitHub, and
1:05
Shopify. Basically, REST gives you the
1:08
full buffet. GraphQL gives you your
1:11
custom meal. SOAP API. Soap, the old
1:16
school business guy in the suit. It's
1:18
super strict and uses XML instead of
1:21
JSON. You'll find it in banking,
1:24
insurance, or government systems.
1:26
Anywhere reliability and security
1:28
matters most. It's slower and more
1:31
complicated, but man, it never breaks
1:34
rules. Think of SOAP like an old bank
1:36
server. Slow, but trustworthy.
1:40
GRPC.
1:41
Now meet gRPC, Google's speed monster.
1:45
It's like REST, but it speaks binary
1:47
instead of JSON, which makes it insanely
1:50
fast for microervices. Small programs
1:53
talking inside big systems. So instead
1:56
of waiting for data like REST does,
1:58
GPRGC just zips it over in lightning
2:01
speed used in big tech systems where
2:03
every millisecond counts. Boom.
2:06
Performance mode on. Web hooks. Let me
2:10
fix a big misunderstanding here. A web
2:12
hook isn't you calling an API, it's the
2:15
API calling you. It's like saying, "Hey,
2:18
API, notify me when something happens."
2:21
For example, when someone buys your
2:23
product, the payment system calls your
2:25
app automatically to say, "Yo, order
2:28
complete." You don't keep asking. It
2:30
tells you first. Boom. Instant updates.
2:33
Zero waiting. Websocket. All right. Now,
2:37
imagine chatting or gaming online.
2:40
That's not REST anymore. You can't keep
2:42
hitting refresh for new messages. That's
2:45
where websockets step in. They keep a
2:47
live connection open between your app
2:49
and the server. So the moment something
2:51
changes, boom, data flies instantly both
2:55
ways. No delays, no reloads, just pure
2:59
realtime magic. Think WhatsApp, Discord,
3:02
or online games. All websocket powered.
3:05
And finally, Web RTC, the cool kid
3:09
behind live calls and video chats. When
3:12
you FaceTime, talk on Google Meet, or
3:14
stream webcam to webcam, that's Web RTC.
3:18
It connects devices directly browser to
3:21
browser. No middleman server.




Introduction
0:00
you intuitively know how important it is
0:03
to get fast responses if you make users
0:06
wait they will bounce off your site
0:08
faster than a kangaro on a trampoline
0:11
however there are so many things we can
0:13
do to improve the performance of our API
0:17
so to not waste your time we will
0:19
discuss only the things that we consider
0:22
the most effective meaning the things
0:25
that require the least amount of effort
0:28
to implement but which would provide the
0:31
greatest performance
HTTP Compression
0:35
benefit we'll start off with HTTP
0:38
compression when done right data
0:40
compression can result in a response
0:43
time that is several times faster
0:46
compared to an uncompressed response but
0:49
how can we determine how much faster is
0:51
the response that depends on the
0:54
compression ratio for instance when the
0:57
response we send is text based such as
1:00
Json XML HTML or plain text then
1:04
compression can reduce the size of the
1:06
data up to 10 times this basically means
1:10
that it will make the response time 10
1:13
times faster if we don't consider the
1:15
time for setting up the connection but
1:17
this will be negligible anyway when
1:20
working with large files binary files
1:23
such as images or videos could also
1:26
benefit from HTTP compression but not as
1:30
much as the text files since binary
1:33
files are usually already compressed
1:36
still applying compression at the HTTP
1:39
level can still reduce the response time
1:42
to half next when should we use HTTP
1:47
compression when the API clients are
1:49
connected over High latency connections
1:52
such as long distance connections or
1:55
even satellite connections then a light
1:58
saer would be the HTTP compression sure
2:02
if you can afford servers in multiple
2:04
locations then you won't have latency
2:07
problems but if you don't want to spend
2:09
money on extra servers then you should
2:12
be really careful with the size of the
2:14
file that you send over the network as
2:17
it will really make a difference another
2:19
use case for when the compression is
2:22
especially beneficial is when you have
2:24
data that is frequently changing in this
2:28
case we can't really profit from a
2:30
caching mechanism as it's basically
2:32
worthless so we need to make use of
2:35
compression to significantly reduce the
2:38
data transfer in such a system last but
2:41
not least if you have to transfer large
2:44
text files then compression will provide
2:47
a substantial benefit the bigger the
2:50
file the more pronounced the benefit the
2:53
only drawback of compression is that it
2:55
requires more CPU resources if your API
2:59
server has limited processing power but
3:02
has generous Network bandwidth then
3:05
enabling compression may not be the best
HTTP Compression: How to?
3:10
choice let's see quickly which are the
3:13
steps to implement compression at HTTP
3:16
level on the server but also on the
3:19
client we'll start with the server side
3:22
or the API provider first we have to
3:25
enable compression on the server most
3:28
popular web servers or server side
3:30
Frameworks provide buildin features or
3:33
plugins to enable compression so all we
3:36
need to do is to enable it for instance
3:39
for engine it can look like this you
3:42
need to consult the documentation that
3:45
is specific to your server framework or
3:48
web server to learn how to enable
3:50
compression then we should set the
3:53
appropriate HTTP headers when responding
3:57
to API requests we need to include the
4:00
appropriate HTTP headers to indicate
4:04
that the compression is enabled here we
4:06
have the content encoding header which
4:09
should specify the compression algorithm
4:12
being used such as gzip or deflate these
4:17
two algorithms are widely supported by
4:20
most HTTP clients then on the client
4:24
side or API consumer we need to include
4:27
an accept encoding header in the the
4:29
HTTP request to indicate the client
4:32
preference for compressed or
4:34
uncompressed responses for example the
4:38
client can said accept encoding gzip
4:41
deflate if it supports compression or
4:45
omit the header if it prefers
4:47
uncompressed responses afterward the
4:50
client needs to handle decompression to
4:53
extract the original response data if
4:56
the client is a browser then this is
4:59
handled
5:00
automatically but if the client is an
5:02
application then you have to instruct
5:05
the HTTP client libraries or Frameworks
5:08
you use to automatically handle the
HTTP Caching
5:14
compression with caching your API
5:17
responses are faster than a cafe cheetah
5:21
since the requests are already processed
5:24
and ready to be served with caching
5:27
responses from server are temporarily
5:30
stored and reused in addition to the
5:33
performance benefit caching will also
5:36
reduce the need for redundant data
5:39
retrieval from the server caching it's
5:42
like giving your server a SP so it
5:44
doesn't get stressed out handling the
5:46
same requests over and over now caching
5:51
can be implemented at any level of the
5:54
interaction but since we are talking
5:56
about rest apis we are going to see how
5:59
caching can be achieved only by using
6:02
some HTTP Logic the advantage of this
6:05
type of caching is that we don't need
6:08
additional systems like caching servers
6:11
since the response would be stored on
6:13
the clients here the client can be a web
6:17
server or an application then client
6:20
would go and ask if the local copy he
6:23
has is the most recent if that specific
6:26
resource hasn't been updated then the
6:29
server would respond with HTTP 304 not
6:35
modified so the client can continue to
6:38
use its local cached copy although this
6:42
kind of conditional requests are invoked
6:44
across the network unmodified resources
6:48
will result in an empty response body
6:51
thus saving the time and Hardware
6:54
necessary for transferring the resource
6:57
back to the end client
HTTP Caching: How to?
7:01
now let's see which are the steps
7:04
involved in implementing a cach
7:06
mechanism on the server side first we
7:09
need to identify cachable resources or
7:13
endpoints in our rest API which are the
7:16
ones that can benefit from our caching
7:20
typically resources that don't change
7:23
very much are good candidates for
7:25
caching static resources are ideal for
7:29
this
7:30
second we need to set the cach control
7:33
header in the API responses the cash
7:36
control header is the most important
7:38
header to set as it basically switches
7:42
on caching on the clients without this
7:45
head the browser will request again the
7:48
file on each subsequent request if set
7:52
to public then resources can be cached
7:55
not only by the end user browser but
7:58
also by any intermediate proxies if set
8:02
to private then resources are not taken
8:05
into consideration by middle proxies and
8:09
can only be cached by the end clients
8:12
additionally we can also include the
8:14
maximum amount of time a resource can be
8:17
cached before considered out of date for
8:21
example max age of one year in seconds
8:25
would look like this finally we need to
8:28
choose a caching strategy based on our
8:32
preferences here we can use the last
8:35
modified header to decide whether a
8:38
resource is still valid or not this is
8:41
time-based expiration strategy or we can
8:46
use an e tag which is basically a digest
8:49
or a hash of the real content then we do
8:53
the validation based on the content now
8:57
let's discuss what we should do if we
9:00
are on the client's side let's say that
9:02
the client makes a request for a
9:05
resource for its first time so that
9:08
resource is not yet cached locally when
9:11
we get the response from the server we
9:14
should first inspect the cash control
9:16
header to know if the response can be
9:19
cached and for how long in this case the
9:23
client will store the resource a
9:25
document file in the local cache for
9:28
subsequent request
9:30
Additionally the server can provide some
9:32
headers which are really helpful for the
9:35
client to let him determine if a
9:37
resource have changed since the last
9:40
request for example last modify header
9:43
or the eag header the client will also
9:47
store this information to use it later
9:50
second step is to use this headers as
9:54
conditional requests after a while the
9:56
client might request again the same
9:59
document but now he can use the
10:02
conditional request to check whether the
10:05
resource was updated since the last call
10:08
if the server supports time based
10:11
expiration then we can use if modified s
10:15
header to validate the resource this
10:19
allows the server to respond with three
10:22
or4 not modified status if the cached
10:25
version is still valid similarly if the
10:29
server supports content-based validation
10:33
then we can use if nonmatch header
10:36
together with an eag to validate if the
10:39
resource have been changed again the
10:42
server responds with 30 or4 not modified
10:46
so we can use the value from the local
10:48
cache this helps a lot to reduce Network
10:52
latency and server load however if the
10:56
server responds with something different
10:59
than 304 for example with a 200 success
11:04
then we need to process the response and
11:07
store it locally and reuse it for
11:10
subsequent
Rate Limiting
11:15
requests rate limiting for rest API is
11:19
putting limits on how many requests a
11:21
client can make in a certain amount of
11:24
time a rate limiter is like a bouncer at
11:27
a club is there to keep things under
11:30
control but how can it help with
11:32
performance when a server is getting
11:35
overloaded and running out of resources
11:38
it can become slow or even unstable
11:41
nobody likes a flaky API server with
11:44
rate limiting you can prevent these kind
11:47
of situations since the number of
11:49
requests is under our control we can
11:52
make guarantees for a service quality
11:55
for example we can guarantee a certain
11:58
response time for each request or we can
12:02
guarantee that the client can make at
12:04
least a certain number of requests per
12:07
unit of time also you want to provide
12:10
Fair performance for each of your
12:13
clients right if a specific client makes
12:17
a large number of requests it might
12:19
monopolize the service and affect the
12:22
performance of other clients but rate
12:25
limiting can help with that by treating
12:28
everyone equ Al so that you can ensure a
12:31
consistent level of service for all of
12:34
your
Rate Limiting: How to?
12:37
users to implement a rate limiter on the
12:40
server side we need to do the following
12:42
steps first we need to define the rules
12:46
here we Define the criteria for rate
12:49
limiting such as maximum number of
12:52
requests allowed per client per endpoint
12:55
or per time window next we choose a rate
12:59
limiting algorithm you should select a
13:01
rate limiting algorithm that suits your
13:04
requirements such as token bucket or
13:07
sliding window each algorithm has its
13:10
own tradeoffs in terms of accuracy
13:13
memory usage and ease of implementation
13:16
we discuss them in detail in the rate
13:18
limiting video next we need to track and
13:22
store request metrics for each client we
13:26
need to keep track of some metrics for
13:28
examp example the number of requests
13:30
made and the time stamp of the last
13:33
request we store this information in a
13:35
data store such as a cache or a
13:39
database finally we enforce the rate
13:42
limits before processing each incoming
13:45
request we check the request metrics for
13:47
the client and we verify if it exceeds
13:51
the defined rate limit rules if the
13:54
limit is exceeded then we respond with
13:57
an appropriate HT TP status code for
14:00
example 4 to9 too many requests or we
14:04
Implement a custom error response now
14:08
let's move on the client side first the
14:11
user client should respect the HTTP
14:14
status code it receives and if there are
14:17
the rate limiting headers as well
14:20
besides the status code the server May
14:23
provide additional rate limiting
14:25
information in the response headers for
14:28
example
14:29
the maximum number of requests per time
14:32
window the remaining request within the
14:35
same time window and the reset time
14:37
stamp the second step is to have a back
14:40
off strategy when the client receives a
14:44
rate limit response for example HTTP
14:47
status code 429 it should handle it
14:50
gracefully a retry logic can be
14:52
implemented here while respecting the
14:55
header called retried after if it is
14:58
provided provided by the
Async APIs
15:03
server asynchronous request can
15:06
significantly improve throughput and
15:08
scalability when multiple requests can
15:11
be processed at the same time without
15:13
waiting for each other our application
15:16
would be much more efficient therefore
15:19
it could handle a higher volume of
15:21
incoming
15:22
requests in this e-commerce website
15:26
example we see that in the synchronous
15:29
method the user is waiting for the First
15:32
Response of the request and he can't
15:35
make a new request while in the asking
15:38
method the user can make other requests
15:40
while waiting for the first request in
15:43
this assing system it looks like the
15:46
requests are processed in parallel
15:48
although technically they are not since
15:51
only one thread is involved
15:54
usually the end result is a much faster
15:57
processing time
15:59
but why is this
16:02
difference for this let's zoom in in the
16:05
e-commerce website that is synchronous
16:08
an e-commerce website normally has a
16:10
shopping cart feature when a user adds
16:14
items to their card and proceed to the
16:16
checkout the website needs to do several
16:19
things to calculate the total price to
16:22
apply discounts if there are promotions
16:25
and to do inventory checks and all these
16:29
for each item in the cart let's say that
16:32
these functionalities are implemented by
16:35
separate services in a synchronous
16:38
method we can execute these three
16:40
operations in two ways we can choose to
16:44
do it sequential which means that the
16:46
caller waits for the response before
16:49
moving on to the next request this is
16:52
quite bad because we have to wait for
16:55
the previous action before we move to
16:58
the next
16:59
the other option is to do it in parallel
17:02
here we'll need to fire up multiple
17:05
threads and then manage them this is not
17:08
ideal because managing a large number of
17:11
threads can consume a significant amount
17:14
of memory and CPU resources multiple
17:17
threads in parallel is an efficient way
17:20
of working when doing intensive
17:22
computation tasks like cryptocurrency
17:25
mining or machine learning in those
17:28
cases is we can actually leverage the
17:31
processing power however when waiting
17:34
idle for responses that should come over
17:37
the network it's much more efficient to
17:40
use async
Async APIs: How to?
17:42
communication now let's see how this
17:45
would work in an asynchronous approach
17:48
for each item the user adds in the card
17:51
we make asynchronous HTTP requests to do
17:54
calculation and inventory checks we do
17:58
this simultaneous L without waiting for
18:00
each response here's what happens
18:03
concurrent requests are made to the
18:06
pricing and Inventory Services for each
18:09
item in the card we see that the website
18:13
doesn't wait for any specific response
18:15
to come back and continues to provide a
18:18
responsive user interface as the
18:21
responses arrive the website updates the
18:24
card total price and inventory status
18:27
for each item
Efficient Serialization
18:32
when it comes to serialization Json and
18:35
XML are the most popular and supported
18:39
across all programming languages and
18:42
platforms quick reminder serialization
18:45
is the process of transforming
18:48
application objects like the ones from
18:50
Java or python into Json or XML file
18:55
formats with the purpose of being stored
18:58
or transferred over the network this
19:01
realization is the opposite process
19:04
where the file representation would be
19:06
transformed in an application object the
19:10
issue with the Json and XML formats is
19:13
that they are not the most efficient way
19:16
of encoding when it comes to Performance
19:19
protocol buffers generally outperforms
19:22
Json and XML in terms of serialization
19:26
and deserialization speed and the
19:29
message size due to its binary format
19:32
and efficient encoding protocol buffers
19:36
can significantly reduce the size of the
19:38
payload and therefore improve the
19:41
network transfer speed in some way
19:44
protocol buffers is similar to http
19:47
compression as they both significantly
19:50
reduce the size of text based
19:54
formats however compression doesn't
19:57
address the inherent inefficiencies of
20:00
those formats protocol buffers is
20:03
treating this at the source still if you
20:07
value more being compatible with
20:09
existing systems or the human
20:12
readability then the text formats with
20:14
compression can still be a workable
Efficient Serialization: How to?
20:19
option now let's see how we can send
20:23
some data between two applications using
20:26
the protocol buffers on the server side
20:29
we need to do the following steps first
20:31
Define the protocol buffer schema we do
20:34
this by creating a Proto file where we
20:37
Define the message types and their
20:40
fields using the protocol buffers
20:43
language syntax this schema will serve
20:46
as the contract between the clients and
20:48
the server the most uncommon elements of
20:52
the Proto files are the numbers assigned
20:55
to each entity of the message this
20:58
dedicated numbers make each attribute
21:00
unique and are used to identify the
21:04
assigned fields in the binary encoded
21:07
output next we will use this person
21:11
Proto file we created to generate code
21:14
in the desired programming language in
21:17
our case we want to generate python
21:19
specific code so we need to use the
21:23
protoc compiler protoc C and give it as
21:27
parameter by python minus out to
21:30
generate python specific code and we
21:33
also specify the Proto file that will be
21:37
compiled the code will generate classes
21:40
and structures that will represent the
21:43
defined message types and will handle
21:46
serialization and
21:48
deserialization next in our code we want
21:52
to serialize the data we want to send
21:55
over the network we use the generated
21:57
code to create and send the protocol
22:01
buffer messages then we create a dammi
22:04
object and then for the rest API it
22:07
often involves serializing messages to
22:10
Binary format and sending them as
22:13
request or response content finally we
22:17
set the appropriate content type header
22:20
this is to express that the response
22:23
data is in protocol buffers format the
22:26
common content type is is application
22:30
SLX minus protuff on the client side the
22:34
procedure is similar first we'll use the
22:38
same person Proto file to generate the
22:41
client code in the desired programming
22:44
language we'll use again the protocol
22:47
buffers compiler protoc C then we set
22:51
the appropriate accept header this is to
22:54
express that the response data is in
22:57
protocol buffers form
22:59
finally we deserialize the response data
23:03
when the client receives a response from
23:05
the rest API it can use the generated
23:08
code to deserialize the binary protocol
23:11
buffers data into the corresponding
23:14
message object now we can access the
23:18
response data to retrieve the desired
23:21
data the client code will also provide
23:24
accessors or properties for the fields
23:27
in the prot tool buffer
Long-running requests
23:32
schema a request that might take a long
23:36
time to process such as a complex
23:38
calculation or bul processing should be
23:42
performed without blocking the client
23:44
that submitted the request if we make
23:46
the clients wait for the response then
23:49
several unwanted things might happen
23:52
first this can reduce scalability
23:55
because of poor resource utilization
23:58
longer timeouts increase the time or
24:01
request can hold resources leading to
24:03
decreased performance and resource
24:06
utilization both on the server and the
24:09
client if a large number of requests
24:12
with long timeouts are waiting for
24:14
responses it can lead to Resource
24:17
exhaustion and reduced scalability for
24:20
instance this can lead to connection
24:22
pool saturation most web servers have a
24:26
limited number of allowed connection
24:28
connections long timeouts can cause
24:31
connection pools to be occupied for
24:34
extended periods limiting the
24:36
availability of connections for other
24:39
incoming requests this can result in
24:42
connection pool exhaustion and prevent
24:45
new requests from being
Long-running requests: How to?
24:48
processed let's see a simple example
24:52
imagine we have a data set that we want
24:54
to give to a server and we want in
24:57
return some calculation done like the
25:00
mean median minimum and maximum let's
25:04
imagine that this is a long running
25:06
process how do we provide asynchronous
25:09
support for this request using rest
25:13
endpoints for this we can return HTTP
25:16
status to O2 accepted to indicate that
25:20
the request was accepted for processing
25:23
but is not completed the web API can
25:26
perform some initial checking to
25:28
validate the request and then initiate a
25:31
separate task to perform the work and
25:34
return a response message with HTTP code
25:37
202 accepted basically we offload the
25:41
request to a background task in addition
25:44
to the post endpoint we should also
25:47
expose an endpoint that Returns the
25:49
status of the asynchronous request so
25:52
that the client can monitor the status
25:55
by pulling the progress at regular in
25:58
intervals of time when the progress
26:00
status shows done we can go ahead and
26:03
check again the initial end point for
26:05
the results a common question is what is
26:08
considered a long response time the
26:12
processing time for long running
26:14
processes can range from seconds to
26:16
hours or even days depending on the
26:19
specifics of the task for any processing
26:23
that takes more than a couple of seconds
26:25
we can implement it using the asnr chos
26:28
mechanism we saw earlier


Introduction
0:00
back in the year 2000 Roy Fielding came
0:03
up with a cool new idea to design web
0:05
services called the rest model since
0:08
then restful web services have become
0:10
the industry standard for building
0:12
modern web applications and services
0:14
knowing how to properly design a rest
0:16
API is one of the most important skills
0:19
a software developer could have and
0:22
there are certain levels of maturity you
0:24
can reach when designing the rest apis
0:27
level 3 corresponds to a truly restful
0:30
API according to fielding's definition
0:32
in practice when you published web apis
0:35
fall somewhere around level 1 or 2.
0:38
reaching level 2 takes practice but it
0:41
will certainly pay off if you want to
0:42
build high quality reliable and scalable
0:45
rest apis and we'll see why reaching
0:48
level 3 is not really feasible in the
0:51
real world
Stateless API
0:53
a rest API should be restful and
0:55
stateless not restless and stateful but
0:58
what does stateless actually means in a
1:01
distributed environment stateless means
1:03
that a client request is not bound to a
1:05
specific server so the servers don't
1:08
maintain any state with the clients
1:10
therefore the client is free to interact
1:13
with any server in a load balanced
1:15
fashion without being tied to a specific
1:18
server in a single server environment
1:20
stateless means that the server can
1:22
process any request without any
1:24
knowledge of the previous requests from
1:27
a certain client so why is this
1:29
important first this makes the API more
1:31
scalable as requests can be processed by
1:34
any available server without relying on
1:36
a specific state from the server also
1:39
this makes the API more available if a
1:42
web server fails then incoming requests
1:44
can be routed to another instance while
1:46
the failed server is restarted with no
1:49
bad effects on the client application
1:51
there are advantages even on single
1:54
instance environment for instance a
1:56
stateless API can be more easily cached
1:59
and optimized this is because the same
2:02
response can be returned for identical
2:04
requests without needing to store any
2:06
state information on the server this can
2:09
improve performance and reduce server
2:11
load even if only one server is being
2:14
used also since there is no need to set
2:17
up complex session state or context
2:19
information the API becomes more modular
2:22
so easier to maintain or test now the
2:26
server can be tested in isolation
Making Stateful Apps Stateless
2:30
okay so the advantages of a stateless
2:33
API are pretty clear but what if we need
2:35
to store State on the application and
2:38
most of the time we'll need to store
2:40
some state to make progress with the
2:42
requests and get some work done for
2:44
instance we may need to maintain the
2:47
state of a shopping cart for e-commerce
2:49
website the client may need to add
2:51
remove or modify items in the shopping
2:54
cart as they navigate through the
2:56
website in this case the server needs to
2:58
maintain some State information in order
3:01
to ensure that the correct items are
3:03
being added or removed from the card and
3:06
the appropriate quantities and prices
3:08
are being calculated to make this app
3:10
stateless we can apply the following
3:13
three steps first we need to identify
3:15
the state of the application so the data
3:18
that is currently being stored or
3:20
managed by the application in this case
3:22
we have item names quantities and prices
3:25
next instead of storing the state within
3:28
the application we need to store it
3:30
externally for example in a database or
3:33
in a cache this ensures that the app
3:35
doesn't rely on any internal State and
3:38
can operate independently
3:40
and third the client would need to
3:42
include a session ID or cookie in
3:45
subsequent request to ensure that
3:47
application is accessing the correct
3:49
card so we use unique identifiers to get
3:53
the state of the card instead of
3:54
utilizing user-specific information to
3:57
keep track of the state this technique
3:59
ensures that any interaction with the
4:02
application can be identified and the
4:04
state can be retrieved accordingly from
4:07
any server and without prior information
4:09
while this approach involves maintaining
4:12
some State information on the back end
4:14
the API is designed to be stateless and
4:17
therefore we can make use of the
4:19
advantages it provides
4:21
so the completed definition of a
4:23
stateless rest API is when each client
4:26
request contains all the necessary
4:28
information needed to process the
4:30
request and the server doesn't maintain
4:32
any session state or context information
4:34
between the requests
No action for the API
4:38
a good API design is organized around
4:41
resources for example customers or
4:44
orders and not actions or verbs for
4:47
instance endpoints like create order
4:49
should be avoided but why is this for
4:51
design first because we have the HTTP
4:54
protocol that brings the action we have
4:57
the HTTP methods all verbs get post put
5:00
patch and delete to handle the actions
5:03
this way we can provide consistency
5:05
between different endpoints and when we
5:08
have consistency clients can make
5:10
assumptions about the behavior of an API
5:13
based on their prior knowledge of the
5:16
HTTP protocol for instance to create an
5:19
order we can use the post or put methods
5:22
on the order resource instead of making
5:24
up different names to achieve the same
5:26
functionality moreover HTTP verbs can be
5:30
mapped to the crude operations of a
5:32
database get means read post means
5:35
create put and Patch means update and
5:38
delete is delete
5:40
that's all nice and pretty but in the
5:43
real world we have more complex use
5:44
cases as well for instance how do we
5:47
model an endpoint that Returns the
5:49
orders made by a customer but allows us
5:52
to sort by an attribute and also to
5:54
paginate the result here we have
5:57
one-to-many relationship and this could
5:59
be represented using path parameters
6:02
however we can improve this API first
6:05
entities are often grouped together into
6:07
collections for example orders or
6:10
customers basically we organize the
6:12
resources as hierarchies which makes the
6:15
API more intuitive and in general it
6:18
helps to use plural nouns for Uris to
6:21
reference collections this provides
6:23
consistent naming for when we need to
6:25
get all the customers or only a
6:27
particular customer then to identify a
6:30
specific user we can use parameterized
6:33
Uris pad parameters are generally
6:35
recommended when you need to specify the
6:38
Identity or the key of a specific
6:40
resource being accessed or modify as
6:43
opposed to query parameters
6:45
then tip number three is to avoid
6:48
resource Uris that are more complex than
6:50
two levels for example customers then
6:54
orders then products is three level deep
6:56
this level of complexity can be
6:59
difficult to maintain and it will not be
7:01
flexible if the relationship between
7:03
resources will change in the future
7:05
instead we could provide two simpler
7:08
Uris for the same requirement now it's
7:11
time to sort the collection for
7:13
additional options or metadata we can
7:15
use Query parameters for instance sort
7:18
by Price query parameters are
7:21
recommended when we need to filter sort
7:23
paginate or when we need to pass
7:25
additional Properties or options to an
7:28
operation so in this case the sort query
7:31
parameter sorts the orders of a customer
7:33
and then the limit parameter specifies
7:36
that only the first 10 matches should be
7:39
returned to summarize in this example we
7:42
use customer as a resource then orders
7:45
as sub resource then we used query
7:47
parameters to get further options on
7:50
those resources
Do not return plain text
7:53
returning plain text for a rest endpoint
7:56
is not a good idea when plain text is
7:58
returned instead of a structured media
8:01
type the client application may have to
8:03
do some extra parsing and processing to
8:06
extract the data it needs this can
8:08
introduce errors and performance issues
8:10
which no one wants we should always
8:13
strive to use Json XML or yaml to
8:16
represent and transmit the data these
8:19
media types provide a structure weight
8:21
of representing data and lets the client
8:23
application easily parse and understand
8:25
the data being returned for rest apis
8:28
you should prefer Json if possible Json
8:31
is widely supported in modern
8:33
programming languages and Frameworks
8:35
better than XML or yaml XML has pretty
8:39
good support however it has unnecessary
8:42
verbosity so using XML can lead to
8:45
larger file size slower parsing and
8:47
increased bandwidth usage yaml is the
8:50
list for both and is more expressive
8:52
than Json however it doesn't have the
8:55
same compatibility across programming
8:57
languages and systems as Json finally
9:00
the API should allow the user to specify
9:03
the content type header for example it
9:05
can be set to the value application
9:07
slash Json if a client sends data in
9:10
Json format the server needs to know
9:12
that it should parse the data as Json
9:15
before processing it or if the client
9:18
specify an unsupported content type the
9:21
server can reject the request preventing
9:23
potential errors or security issues so
9:26
specifying the content type allows the
9:28
client and the server to communicate
9:30
efficiently and understand the format of
9:32
the data being exchanged
Recipe for disaster
9:36
changing RS API after it has been
9:38
adopted by several clients is without a
9:41
question one of the worst thing we can
9:43
possibly do when we suddenly change the
9:45
API clients will find out the hard way
9:48
that the API they've been using is not
9:50
working anymore so their code is broken
9:53
their applications are failing and their
9:56
users are probably angry clients have
9:59
trusted your API to remain stable and
10:01
predictable and you have betrayed that
10:04
trust not cool in this case the clients
10:07
will need to update the documentation
10:08
modify the client code and provide
10:10
support to their clients who are
10:12
struggling to adapt to the new API and
10:15
this should be done as soon as possible
10:17
to make the clients work as before in
10:20
short changing a rest API after it has
10:22
been adopted by several clients is a
10:25
recipe for disaster so unless you have a
10:28
very good reason for doing so and the
10:30
solid plan for managing the transition
10:32
you should never do it however web apis
10:35
ain't gonna State the forever business
10:37
requirements change all the time so you
10:40
might need to add new stuff change how
10:42
stuff is related or tweak the structure
10:44
of the data so how do we give the chance
10:47
to client application to use new
10:49
features and resources while making sure
10:52
that the existing features work as
10:54
before one approach is to make the API
10:56
Backward Compatible for example you
10:59
might have an API that Returns the
11:01
product details and later you would need
11:04
to include the price currency without
11:06
breaking the parsing of existing clients
11:08
you can add an optional parameter
11:10
currency this allows different clients
11:12
to choose whether they need the new
11:14
field and to specify the currency type
11:16
however if we use many times this method
11:20
the end result will be a messy set of
11:22
switches and codes required for each
11:25
call so this might be an option but only
11:27
for quick and simple updates however the
11:30
common way to update an API is by
11:33
versioning we can specify the version of
11:35
the API in the URI by appending query
11:38
parameters by adding HTTP headers or by
11:41
specifying media types but which one
11:44
should we choose let's see the benefits
11:46
and trade-offs considering the
11:48
performance the URI versioning and query
11:50
string versioning are cache friendly
11:52
usually they refer to the same data each
11:55
time when an URI or a query string
11:58
combination is used these two approaches
12:00
are pretty common however from a restful
12:03
perspective the URL should not be
12:05
different depending on the version when
12:07
fetching the same data so for rest
12:10
periods we have the options to specify
12:13
the version using a custom or an accept
12:15
header these are less intrusive since
12:18
they don't change the url ultimately the
12:21
choice of the API versioning depends on
12:23
development team preferences but
12:26
normally URI versioning are the easiest
12:28
to understand and Implement while media
12:31
type might be considered the purest of
12:34
the versioning mechanism which can also
12:36
support hated videos
Handling Exceptions
12:39
when designing a rest API we have to
12:42
make sure we add solid exception
12:44
handling we don't want uncode exceptions
12:47
to sleep and propagate to the client for
12:50
example if a user makes a request for
12:52
the user details then the API might
12:54
require to have the user ID as a number
12:56
instead of throwing a generic error
12:59
message and the status code of 500 for
13:02
everything we should let the user know
13:04
exactly what is wrong so we should catch
13:07
the exception and wrap it with a
13:09
descriptive message and an appropriate
13:11
HTTP status code here is really
13:14
important to make sure that you are
13:16
using the right status code for the
13:18
situation if you're confused about what
13:20
status code to use you can always check
13:22
the status code definition page
13:24
published by the standards organization
13:26
ietf
13:28
make sure that you distinguish between
13:31
client-side errors and server side
13:33
errors client errors normally require
13:36
the client to make some changes on the
13:39
request while server errors are the
13:42
responsibility of the application to
13:43
address and solve this also makes it
13:46
easier for the application to identify
13:49
the real problems on the server by
13:51
monitoring the 5xx errors finally a pro
13:55
tip is to make use of a global error
13:58
handling strategy across the entire web
14:01
API this will improve the user
14:03
experience by providing clear and
14:05
consistent error messages also using a
14:09
global error handling approach can help
14:11
to manage the complexity and makes the
14:13
API more scalable a global error
14:16
handling approach can be reused across
14:18
different API endpoints reducing the
14:21
amount of duplicate code needed to
14:23
handle errors
Worth to HATEOAS?
14:27
the last level of rest API maturity is
14:30
to use hypermedia or haters for example
14:33
to handle the relationship between an
14:35
order and the customer the
14:36
representation of an order could include
14:38
links that identify the available
14:40
operation in that order that could be
14:43
made by a customer this principle is a
14:46
way for the API to describe the state of
14:48
the application and to provide links to
14:50
the next available actions that can be
14:52
performed by the user or client
14:54
application this principle sounds nice
14:57
in theory because it provides a
14:59
discoverable and self-descriptive API
15:01
which allows the server to change the
15:03
Uris without breaking clients however it
15:07
has some major disadvantages first
15:09
performance concerns including links to
15:12
related resources and actions can
15:14
increase the size of the API responses
15:16
which can impact the performance this is
15:19
especially true for apis where many
15:22
requests are made for the same resource
15:24
for instance millions of requests Inc
15:27
including the links in such an example
15:28
is a waste of resources second there is
15:31
a lack of standardization for this
15:33
principle currently there is no widely
15:36
accepted standard for implementing ATS
15:38
in rest apis because of these reasons
15:41
and others this concept has a low
15:43
adoption and remains more a theoretical
15:46
principle and not usually put in
15:48
practice


Introduction – Why Caching is Essential for REST APIs
0:03
caching is like keeping a shortcut to
0:05
frequently traveled paths it saves time
0:08
and resources in the world of apis
0:10
caching can dramatically improve
0:12
performance and scalability by reducing
0:14
the load on databases and servers but
0:17
how do we Implement caching effectively
0:18
in rest apis let's go step by step
0:21
starting with the
0:25
[Music]
0:28
basics the application layer is where
Application Layer Caching – Using Redis for Fast Data Retrieval
0:31
most caching happens in Rest apis by
0:33
caching frequently accessed data we can
0:35
cut down on redundant database queries
0:37
and computations making our API much
0:40
faster inmemory caching is a great first
0:43
step tools like redis and M cach are
0:46
popular choices they store data in
0:48
memory making retrieval almost
0:50
instantaneous and here is a simple Java
0:53
example where we are using redis an
0:55
in-memory data store to cach user
0:57
profiles and minimize database hits here
1:00
the redis client. getet user ID
1:02
retrieves the profile from redis using
1:04
the user ID as the key if the data exist
1:07
it's a cash hit and the profile is
1:09
returned instantly no need to query the
1:12
database if the data isn't found in
1:14
reddis it's a cash Miss in this case the
1:17
application fetches the profile from the
1:19
database this is the fallback mechanism
1:22
the database query ensures that the user
1:24
still gets the correct data even when
1:26
the cache is empty once the data is
1:28
retrieved from the database we cach it
1:30
in redes for future request here the
1:33
setex method stores the profile in redes
1:35
with a time to live or TTL of 300
1:38
seconds or 5 minutes this means the data
1:41
will automatically expire after 5
1:43
minutes ensuring the cache doesn't hold
1:45
outdated
1:47
information now think about the next
1:49
request for the same user profile
1:52
instead of hitting the database again
1:54
the application fetches it directly from
1:56
radius in milliseconds this reduces
1:58
latency minimizes load on your database
2:01
and improves the overall user experience
2:03
by using redis as a caching layer in
2:05
your rest API you can handle higher
2:07
traffic reduce response times and lower
2:10
the load on your database it's a simple
2:12
yet powerful technique to make your
2:14
application scalable and efficient
2:16
application layer caching happens deeper
2:18
within the application logic often
2:20
focusing on specific data or computation
2:23
results rather than entire responses
Request-Level Caching – Storing Full API Responses
2:26
request level caching is all about
2:27
caching entire API responses for
2:29
specific request it's tied to individual
2:31
API calls and is typically implemented
2:34
based on unique request parameters for
2:36
example query strings or request headers
2:39
this approach is especially useful for
2:40
read heavy operations like get request
2:43
where the data doesn't change frequently
2:45
and here is the basic workflow when a
2:47
client makes a get request the server
2:50
first checks if a cache response exist
2:52
if it does then it's a cache hit the
2:55
server immediately Returns the cache
2:56
data if it doesn't it's a cache miss the
2:59
server processes the request generates
3:01
the response and caches it for future
3:04
use request level caching relies on
3:06
generating unique cache keys for each
3:08
combination of request parameters a cach
3:11
key is a unique identifier used to store
3:13
and retrieve cache data it represents a
3:16
specific request for data set and
3:18
ensures that cash responses are
3:20
correctly associated with a
3:21
corresponding request this ensures that
3:24
responses for different queries don't
3:26
override each other for example consider
3:28
an API that fetches paginated user list
3:31
so let's say you have an API to fetch
3:33
user details here you can use the user
3:36
ID as a key this ensures that the cat
3:38
response is tied specifically to user
3:40
123 a request for user 456 would use a
3:44
different key say user
3:46
456 or you can consider an API that
3:49
returns a paginated list of users here
3:52
use a combination of the query
3:53
parameters to create the key for example
3:57
consider an API method that fetches
3:58
paginated user link list here we first
4:01
generate a unique key user listor pageor
4:04
2core limitor 10 it ensures that the
4:07
response for each request variation is
4:09
stored
4:09
separately redis client. getet cach key
4:13
checks if the response is already cached
4:15
if it's a cach it the response is
4:17
returned immediately and if it's a cache
4:19
Miss meaning if the data isn't found in
4:21
redis the database is qued using fetch
4:24
users from database page comma limit and
4:27
once the data is retrieved it is stored
4:28
in red with a TTL of 600 seconds or 10
4:31
minutes this ensures the stale data is
4:34
automatically removed request level
4:36
caching is ideal for read heavy apis
4:39
these are the apis with frequent get
4:40
request and relatively static data and
4:43
end points that involve relatively
4:45
complex computations for large database
4:46
queries for example responses with
4:48
parameters like page or
4:50
limit now what if the client only needs
Conditional Caching – ETag & Last-Modified for Efficient API Calls
4:53
data that has changed since their last
4:55
request conditional caching is a
4:57
technique that ensures clients receive
4:58
updated data only when necessary
5:01
minimizing redundant data transfers it's
5:03
an efficient way to reduce bandwidth
5:05
usage and improve API responsiveness by
5:07
leveraging HTTP headers like eag and
5:10
last modified eag is a unique identifier
5:13
for resource version and last modified
5:16
is the timestamp of the last update to a
5:18
resource let's see how this works here
5:21
the server first calculates an eag for
5:23
the resource using the hash of its data
5:26
the current eag acts as a unique
5:28
identifier for the specific version
5:30
of the user data on subsequent request
5:33
the client sends the E tag in the if non
5:35
match
5:36
header the server compares the client if
5:39
non match value with the current eag and
5:41
if they match it means the data hasn't
5:43
changed the server responds with a 34
5:46
not modified status and no response body
5:48
saving
5:49
bandwidth if the eag doesn't match or
5:52
isn't provided the server Returns the
5:54
full resource and the new eag the client
5:56
then stores the new eag for future
5:58
request and here is a sample client
6:01
server interaction in the first request
6:03
there is no eag sent by the client and
6:06
the server responds with a 200 okay and
6:08
a response body in the subsequent
6:11
request client sends an eag and the
6:13
server checks for the eag if unchanged
6:16
it sends the response at 304 not
6:19
modified and if it is changed it sends a
6:21
response with 200 okay and headers with
6:24
new eag and updated data in the body
6:27
this showcases how eag based conditional
6:29
caching minimizes unnecessary data
6:31
transfer and ensures clients always get
6:34
the latest data when needed by
6:36
leveraging eag and last modified headers
6:38
it ensures faster responses lower
6:41
bandwidth usage and an overall better
6:43
user experience in rest apis now as we
6:46
have learned caching is powerful but
Cache Invalidation – Write-Through, Write-Behind & TTL-Based Eviction
6:48
scale or outdated data can lead to
6:51
inconsistencies so to ensure users get
6:54
accurate data we need strategies for
6:56
cache invalidation that is the methods
6:58
to update or remove cache data when the
7:01
underlying sources changes let's explore
7:04
three common approaches with detailed
7:05
examples including the use of redis in
7:08
the write through strategy the cache is
7:10
updated synchronously whenever the
7:11
database is updated this ensures that
7:14
the cache always holds the latest
7:16
data so here the application writes the
7:19
data to both the database and the cache
7:21
at the same time the cache stays in sync
7:24
with the database so reads can be served
7:27
directly from the cache the main benefit
7:29
of this approach is that it ensures the
7:31
cach is always up to date it's simple to
7:33
implement for read heavy systems but
7:35
it's slightly slow to ride due to
7:37
synchronous nature of cache updates and
7:40
so every database triggers a cache
7:42
update in the right behind strategy the
7:44
cache is updated asynchronously after
7:47
the database is updated the database
7:49
Remains the source of truth and the
7:51
cache update happens in the background
7:53
as you can see in the code here the
7:55
application writes to the database first
7:57
and then a background process for for
7:59
example a q or thread updates the cache
8:02
after the right is complete and so it
8:04
has faster rights since cache updates
8:06
are differred and suitable for high
8:08
right throughput systems but again cache
8:12
might temporarily hold steel data until
8:14
the update is complete and it is also
8:16
more complex to implement due to the
8:18
asynchronous handling and finally we
8:21
have TTL based eviction in this approach
8:24
cash data is automatically expired after
8:26
a set time to live or TTL once expired
8:29
the the cach either fetches fresh data
8:31
from the database or deletes the entry
8:33
so here data is returned to the cach
8:35
with a TTL value and when the TTL
8:37
expires the cash entry is removed or
8:39
refreshed for example here is how TTL
8:42
based eviction Works in redis here the
8:45
data expires in 5 minutes ensuring it
8:47
doesn't linger longer than it should now
8:50
the choice of invalidation strategy
8:51
depends on your system requirements a
8:54
ride through strategy is best for read
8:56
heavy workloads where cash accuracy is
8:58
critical but it has slow rides a right
9:01
behind is for right heavy workloads
9:04
where cash freshness can tolerate
9:06
deletes but again there is a temporary
9:09
cach tailess and TDL base is data with
9:12
predictable or time sensitive expiration
9:14
for example product prices or session
9:16
tokens but it still has potential for
9:18
stale data within the
9:20
TTL cash invalidation ensures that your
9:22
cash data stays fresh and accurate
9:25
avoiding stale responses that can
9:27
frustrate your users by carefully Cho
9:29
using the right strategy right through
9:31
right behind or tail based eviction you
9:33
can balance performance and consistency
9:35
to meet the needs of your
9:37
application caching becomes truly
9:39
powerful when implemented across
9:41
multiple layers of the system each
9:43
designed to handle specific types of
9:44
data closer to the user this approach
9:47
optimizes performance minimizes server
9:49
load and delivers faster responses let's
9:51
break this down step by step with an
9:53
example so imagine a user is requesting
Layered Caching – Browser, CDN & Server-Side Optimization
9:56
a product image on an e-commerce website
9:59
the Journey of this request involves
10:01
multiple layers of caching when the
10:02
user's browser first loads the image it
10:04
stores it in the cache if the user
10:06
revisits the page or navigates back the
10:09
browser retrieves the image locally
10:11
without even contacting the user this is
10:14
the fastest layer of caching and ensures
10:16
instant access for repeated
10:18
request if the browser cache doesn't
10:20
have the image the request is forwarded
10:22
to a CDN or content delivery Network a
10:25
CDN is a network of servers distributed
10:28
globally designed to cach and serve
10:30
content like images videos and static
10:32
files for example if the user is in
10:34
London the image might be served from a
10:36
CDN server in the UK rather than the
10:39
origin server reducing latency and
10:41
server load and if neither the browser
10:44
nor the CDN has the image the request
10:46
reaches to your application's API server
10:48
here an application layer cache like
10:50
redis or mam cach stores the metadata or
10:53
frequently access data about the image
10:56
for instance the API server could cat
10:58
metadata like image URL resolution or
11:01
compression format this reduces the need
11:03
of repetitive database queries ensuring
11:06
the back end remains efficient and
11:08
finally if the cache misses at all lers
11:11
the server fetches the image metadata
11:12
from the database this is the last
11:15
resort and typically the slowest step
11:17
once the data is retrieved it passed
11:19
through the layers updating the caches
11:21
for future
11:22
request guys I have also explored CDN
11:25
and application layer caching in detail
11:27
in my previous videos these topics are
11:29
part of comprehensive playlist which you
11:30
can find in the playlist Linked In My
11:33
description so bringing it all together
Bringing It All Together – Best Practices for Scalable APIs
11:37
with the techniques we have explored so
11:38
far you use in-memory caching for
11:40
frequently Access Data you implement
11:43
request level caching for predictable
11:45
get responses we can leverage
11:47
conditional caching for upto-date
11:48
bandwidth efficient interactions ensure
11:51
consistency with robust cache
11:52
invalidation strategies and combine all
11:55
layers such as browsers CDN and
11:57
application for the ultimate performance
12:00
with this blueprint you can design rest
12:02
apis that are not only fast but also
12:04
scalable and production ready
12:08
[Music]

REST API
0:00
REST API. What is a REST API? Well,
0:05
think of REST API like a waiter at a
0:07
restaurant. You tell the waiter what you
0:09
want, they go to the kitchen, get it,
0:10
and bring it back to you. That's exactly
0:12
what a REST API does between your app
0:14
and a server. REST stands for
0:17
representational state transfer. Fancy
0:20
words that basically mean a simple way
0:22
for applications to talk to each other
0:24
over the internet. REST uses the same
0:26
HTTP methods you already know from web
0:29
browsing. Get to retrieve data like
0:32
asking show me all users. Post to create
0:35
something new like add this new user.
0:38
Put to update existing data and delete
0:41
to remove it. Let's say you're building
0:43
a social media app. When you want to
0:46
show user profiles, your app sends a get
0:48
request to something like
0:50
api.mmyapp.com/users/1
0:54
and boom, you get back all of J's
0:57
profile info in a nice, clean JSON
1:00
format. REST is stateless, means each
1:03
request is completely independent. The
1:05
server doesn't remember your previous
1:07
requests, which means it can handle
1:09
millions of users without getting
1:10
confused about who asked for what. Rest
1:14
is also platform independent. Your
1:16
iPhone app, Android app, web app, and
1:19
even your smart fridge can all talk to
1:21
the same REST API. But REST isn't formal
1:24
enough for your bank transfers and
1:26
enterprise systems. That's where SOAP
1:29
API comes in. SOAP API. SOAP or simple
SOAP API
1:33
object access protocol is one of the
1:36
oldest and most formal ways that systems
1:38
communicate with each other. If REST is
1:41
like a casual phone call, SOAP is more
1:43
like a formal business contract. Every
1:45
message must follow strict rules and
1:48
comes wrapped in XML with a very
1:50
specific structure, an envelope to wrap
1:53
everything, a header for metadata, and a
1:55
body that holds the actual request or
1:57
response. One of SOAP's strengths is
2:00
that it's protocol independent. While
2:02
it's most commonly used over HTTP or
2:05
HTTPS,
2:06
it can also run on SMTP, TCP, or other
2:10
protocols. And because it has built-in
2:12
standards for error handling, security,
2:15
and transaction support, SOAP is often
2:18
trusted in industries where reliability
2:20
and precision matter most. That's why
2:23
banks, healthcare providers, and
2:25
government systems still rely heavily on
2:27
SOAP today. For example, when you
2:30
transfer money between banks, there's a
2:32
good chance a SOAP API is working behind
2:35
the scenes, ensuring the transaction is
2:37
secure and correctly processed. SOAP
2:40
might not be as lightweight or flexible
2:42
as REST, but when you need guaranteed
2:44
delivery, strict contracts, and
2:46
enterprisegrade reliability, SOAP is the
2:49
go-to choice. GRPC API. Before we talk
gRPC API
2:53
about gRPC, let's step back to RPC or
2:56
remote procedure call. RPC is the idea
2:59
that instead of sending raw data over
3:01
the network, your app can directly call
3:03
a function on another machine as if it
3:05
were local. For example, you write get
3:08
user 123 in your code and behind the
3:11
scenes that request travels across the
3:12
network, runs on the server and returns
3:15
the result. But early RPC systems like
3:18
XML, RPC or JSON RPC had problems. They
3:22
were slower, textheavy and didn't scale
3:24
well for today's massive realtime apps.
3:28
That's where gRPC comes in. GRPC is
3:31
Google's higherformance modern take on
3:33
RPC. Think of it as the Formula 1 race
3:36
car of APIs built for speed,
3:38
performance, and precision. While REST
3:41
sends textbased JSON over HTTP, gRPC
3:45
uses protocol buffers or protobu, which
3:48
compress data into a compact binary
3:50
format that's lightning fast to process.
3:53
It's like the difference between mailing
3:55
a handwritten letter versus zipping all
3:57
your files and sending them instantly.
4:00
GRPC also takes advantage of HTTP2,
4:03
allowing multiple requests to run over a
4:06
single connection at the same time. And
4:09
here's the best part. GRPC supports four
4:11
powerful communication patterns. Simple
4:14
request response just like REST. Server
4:17
streaming for live updates, client
4:19
streaming for sending continuous data.
4:22
and birectional streaming where both
4:24
sides can chat at once in real time. The
4:28
performance gains are massive, often 7
4:30
to 10 times faster than rest in many
4:32
scenarios. That's why gRPC is the secret
4:35
weapon behind systems like Netflix,
4:38
Uber, and highfrequency trading
4:39
platforms. GraphQL API. GraphQL stands
GraphQL API
4:44
for graph query language, and it's about
4:46
to change how you think about APIs
4:48
forever. Created by Facebook, GraphQL is
4:51
the game-changing query language that's
4:53
revolutionizing how we fetch data.
4:55
Here's the problem that GraphQL solves.
4:57
In the case of REST APIs, you often get
5:00
too much or too little data. Need a
5:03
user's name and email? Rest might send
5:06
you their entire profile, address,
5:08
preferences, and shopping history.
5:10
That's called overfetching, and it
5:12
wastes bandwidth. Or worse, you might
5:14
need to make multiple API calls to get
5:16
everything you need. That's
5:18
underfetching.
5:19
GraphQL flips this completely. You write
5:22
one query asking for exactly what you
5:24
want, like just give me the username and
5:26
email, skip everything else. One end
5:29
point, one request, perfect data every
5:31
time. But the killer feature of GraphQL
5:34
is real-time subscriptions. Your app can
5:36
listen for live update automatically.
5:38
Plus, GraphQL is self-documenting with a
5:41
built-in playground where you can test
5:42
queries instantly. GitHub's entire API
5:46
is built on GraphQL. Shopify and
5:49
Pinterest all use GraphQL in production.
5:52
If you're building modern applications
5:54
where performance matters, users are on
5:56
mobile devices, and you want to give
5:58
front-end developers the flexibility to
6:01
request exactly what they need. GraphQL
6:04
might just be your new best friend. But
6:06
sometimes you don't just want to fetch
6:08
data, you want to be notified the
6:09
instant something changes. That's where
6:11
web hooks come in. Web Hook API. Imagine
WebHooks API
6:15
this. With most APIs, your app is like
6:17
someone constantly checking their
6:19
mailbox. You walk outside, open it, see
6:21
nothing, and walk back over and over
6:24
again. That's how traditional APIs work.
6:27
Your app has to ask the server every
6:29
time it wants to know if something new
6:30
happened. But web hooks flip that model
6:33
completely. Instead of you asking, the
6:35
API calls you. It's like the mailman
6:37
ringing your doorbell the moment a
6:39
letter arrives. Instant, direct, and
6:41
efficient. Let's see how it works. You
6:43
set up a callback URL in your
6:45
application. Whenever an event happens,
6:48
like a new payment, a code push, or a
6:50
form submission, the service sends a
6:52
post request with the event details
6:54
straight to your callback URL. No
6:56
polling, no wasted requests, just
6:59
real-time updates. That's why web hooks
7:01
are often called reverse APIs. Instead
7:04
of your app chasing the data, the data
7:06
comes chasing your app. You'll find web
7:09
hooks powering nearly every modern
7:10
application like GitHub fires web hooks
7:13
when new code is pushed. Shopify
7:16
triggers them when an order is placed.
7:18
Slack and Discord bots rely on web hooks
7:21
for commands and real-time reactions.
7:23
From automating workflows to keeping
7:25
systems instantly in sync, web hooks are
7:28
the invisible backbone of real time web
7:30
development. Websockets API. Websockets
WebSockets API
7:34
are like opening a permanent phone line
7:36
between your app and the server. Once
7:38
the connection is established, both
7:40
sides can talk to each other anytime
7:42
instantly. No more waiting for the
7:44
client to ask a question like with
7:46
traditional HTTP.
7:48
Here's how it works. It starts with a
7:50
handshake. Your browser sends a special
7:52
HTTP request saying, "Let's upgrade this
7:55
to a websocket connection." The server
7:57
agrees. They shake hands. And from that
8:00
point on, the channel stays open. This
8:02
gives you a persistent two-way
8:04
communication line, which is perfect for
8:06
real-time applications. Unlike regular
8:09
HTTP where the client always initiates,
8:12
websockets allow the server to push data
8:14
to you the moment something happens.
8:17
Think about getting a stock price
8:18
update, a chat message, or a game event
8:20
the instant it occurs. That's the power
8:22
of websockets. And it's flexible, too.
8:26
You can send plain text, JSON for
8:28
structured data, or even binary files
8:30
like images and videos. Web RTC API. Web
WebRTC API
8:35
RTC or Web Realtime Communication isn't
8:38
just a single API. It's a full framework
8:41
that enables direct peer-to-peer
8:43
communication between browsers or mobile
8:45
apps. And here's the magic. The data
8:47
doesn't need to flow through a central
8:49
server. That's why Web RTC powers things
8:52
like video calls, screen sharing, online
8:55
gaming, and instant file transfers. All
8:57
happening right inside your browser with
9:00
no extra software.
9:02
Think about your last Zoom or Google
9:04
Meet Call. When you're talking, your
9:06
video and audio are sent straight from
9:08
your device to the other person's
9:10
device. No server in the middle storing
9:13
or processing your private conversation.
9:16
It's direct and real time. Behind the
9:20
scenes, WebRTC takes care of the messy
9:22
networking details. It figures out NAT
9:25
traversal so devices can talk across
9:27
different networks. It automatically
9:29
negotiates the best audio and video
9:31
formats. And it uses adaptive bit rate
9:34
streaming, which means it constantly
9:36
adjusts quality depending on your
9:37
internet speed. The result, no server
9:40
bottlenecks, faster communication, and
9:43
smoother realtime experiences.
9:45
That's why WebRTC is the backbone of
9:48
modern video conferencing, realtime
9:50
collaboration tools, and peer-to-peer
9:52
apps.
